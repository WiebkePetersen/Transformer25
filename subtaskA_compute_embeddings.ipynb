{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computes BERT embeddings \n",
    "\n",
    "This notebook reads in the data (original data plus gpt generated data) and generates embeddings using different pooling methods. The embeddings are saved in a pandas dataframe in a pkl-file. The name of the pkl-file depends on the settings (with or without preprocessing, used model, ...). Additionally, embeddings with sBERT are generated. The pkl-file is loaded from the later notebooks.\n",
    "\n",
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter setting\n",
    "\n",
    "# model\n",
    "checkpoint = 'bert-base-uncased' \n",
    "#checkpoint ='jlsalim/bert-uncased-idiomatic-literal-recognizer'\n",
    "\n",
    "# full sentences or preprocessed (filtered to content words of specific POS)\n",
    "preprocessed = False \n",
    "#preprocessed = True\n",
    "\n",
    "# remove CLS and SEP tokens from the input (not used in final model as no improvement)\n",
    "#remove_CLS_SEP = True \n",
    "remove_CLS_SEP = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read tsv file\n",
    "import csv\n",
    "import sys\n",
    "import ast\n",
    "\n",
    "import pandas as pd\n",
    "dataDirectory = \"./data/\"\n",
    "\n",
    "# read in competition data\n",
    "dataA_train = pd.read_csv(dataDirectory + \"subtask_a_train.tsv\", sep='\\t')\n",
    "dataA_train['expected_order'] = dataA_train['expected_order'].apply(ast.literal_eval)\n",
    "dataA_dev = pd.read_csv(dataDirectory + \"subtask_a_dev.tsv\", sep='\\t')\n",
    "dataA_test = pd.read_csv(dataDirectory +\"subtask_a_test.tsv\", sep='\\t')\n",
    "dataA_xe = pd.read_csv(dataDirectory + \"subtask_a_xe.tsv\", sep='\\t')\n",
    "\n",
    "dataA = pd.concat([dataA_train,dataA_dev,dataA_test,dataA_xe])\n",
    "# reset index\n",
    "dataA = dataA.reset_index(drop=True)\n",
    "\n",
    "# read in chatGPT data from csv\n",
    "data_chatGPT_train = pd.read_csv(dataDirectory + \"chatGPTNew_train.csv\")\n",
    "data_chatGPT_dev = pd.read_csv(dataDirectory + \"chatGPTNew_dev.csv\")\n",
    "data_chatGPT_test = pd.read_csv(dataDirectory + \"chatGPTNew_test.csv\")\n",
    "data_chatGPT = pd.concat([data_chatGPT_train,data_chatGPT_dev,data_chatGPT_test])\n",
    "\n",
    "\n",
    "data_chatGPT = data_chatGPT.reset_index(drop=True)\n",
    "\n",
    "# rename each column with \"gpt_\" in front of the column name\n",
    "data_chatGPT.rename(columns=lambda x: 'gpt_' + x, inplace=True)\n",
    "\n",
    "# inserting the missing compound column\n",
    "data_chatGPT[\"compound\"] = [None for i in range(len(data_chatGPT))]\n",
    "for i in range(len(data_chatGPT)):\n",
    "    data_chatGPT[\"compound\"][i] = data_chatGPT[\"gpt_idiomatic_meaning\"][i].split(\" is\")[0].strip().lower()\n",
    "\n",
    "# read in gpt image description data\n",
    "data_gpt_image = pd.read_csv(dataDirectory  + \"gpt_image_descriptions_all.csv\", sep=',')\n",
    "\n",
    "# merge data into one dataframe\n",
    "dataA = pd.merge(dataA, data_chatGPT, on='compound')\n",
    "dataA = pd.merge(dataA, data_gpt_image, on='compound')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not data_gpt_image.shape[1] + data_chatGPT.shape[1] + dataA_train.shape[1] - 2 == dataA.shape[1]:\n",
    "    print(\"There is a problem with the merged file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cutting gpt-meaning data\n",
    "def cut_gpt_meaning(sent):\n",
    "    if \"literal\" in sent:\n",
    "        # delete everything up to \"literal\"\n",
    "        sent = sent.split(\"literal\")[1]\n",
    "    elif \"metaphor for\" in sent:\n",
    "        # delete everything up to \"metaphor for\"\n",
    "        sent = sent.split(\"metaphor for\")[1]\n",
    "    return sent.strip()\n",
    "\n",
    "dataA['gpt_idiomatic_meaning_cutted'] = dataA['gpt_idiomatic_meaning'].apply(lambda x: cut_gpt_meaning(x))\n",
    "dataA['gpt_literal_meaning_cutted'] = dataA['gpt_literal_meaning'].apply(lambda x: cut_gpt_meaning(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_type_columns = ['sentence', \n",
    "                         'image1_caption', 'image2_caption', 'image3_caption', 'image4_caption', 'image5_caption', \n",
    "                         'gpt_idiomatic_meaning', 'gpt_literal_meaning',\n",
    "                         'gpt_idiomatic_meaning_cutted', 'gpt_literal_meaning_cutted', \n",
    "                         'gpt_idiomatic_sentence', 'gpt_literal_sentence',\n",
    "                         'gpt_idiomatic_image', 'gpt_literal_image']\n",
    "\n",
    "\n",
    "sentence_with_compound_columns = ['sentence', \n",
    "                         'gpt_idiomatic_meaning', 'gpt_literal_meaning', \n",
    "                         'gpt_idiomatic_sentence', 'gpt_literal_sentence']\n",
    "\n",
    "\n",
    "# cleanup data\n",
    "# replace ’ with ' in all columns\n",
    "for column in sentence_type_columns:\n",
    "    dataA[column] = dataA[column].str.replace(\"’\",\"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing of text \n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download necessary NLTK data\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "def prepare_text(raw_text):\n",
    "    \n",
    "    # Convert text to lowercase and remove punctuation\n",
    "    normalized_text = raw_text.lower()\n",
    "    normalized_text = re.sub(r\"[^a-zA-Z\\s]\", \"\", normalized_text)\n",
    "\n",
    "    # Tokenize the normalized text\n",
    "    tokens = word_tokenize(normalized_text)\n",
    "\n",
    "    # Apply POS tagging and retain only nouns, verbs\n",
    "    pos_tags = nltk.pos_tag(tokens, tagset='universal')\n",
    "    pos_tags_to_keep = {\"NOUN\", \"VERB\", \"ADJ\"}\n",
    "    filtered_tokens = [word for word, pos in pos_tags if pos in pos_tags_to_keep]\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_text = [word for word in filtered_tokens if word.lower() not in stop_words]\n",
    "\n",
    "    # Lemmatize the remaining tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_text = [lemmatizer.lemmatize(word) for word in filtered_text]\n",
    "    \n",
    "    return \" \".join(lemmatized_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if preprocessed == True:\n",
    "    dataA[\"compound\"] = dataA[\"compound\"].apply(prepare_text)\n",
    "    for column in sentence_type_columns:\n",
    "        dataA[column] = dataA[column].apply(prepare_text) \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data analysis**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Extended Evaluation    100\n",
       "Train                   60\n",
       "Dev                     15\n",
       "Test                    15\n",
       "Sample                  10\n",
       "Name: subset, dtype: int64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataA[\"subset\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample\n",
      "number 10\n",
      "ratio literal 0.5\n",
      "ratio idiomatic 0.5\n",
      "===\n",
      "Train\n",
      "number 60\n",
      "ratio literal 0.43\n",
      "ratio idiomatic 0.57\n",
      "===\n",
      "Dev\n",
      "number 15\n",
      "ratio literal 0.53\n",
      "ratio idiomatic 0.47\n",
      "===\n",
      "Test\n",
      "number 15\n",
      "ratio literal 0.47\n",
      "ratio idiomatic 0.53\n",
      "===\n",
      "Extended Evaluation\n",
      "number 100\n",
      "ratio literal 0.54\n",
      "ratio idiomatic 0.46\n",
      "===\n",
      "Sample\n",
      "number 10\n",
      "idiomatic 5\n",
      "literal 5\n",
      "===\n",
      "Train\n",
      "number 60\n",
      "idiomatic 34\n",
      "literal 26\n",
      "===\n",
      "Dev\n",
      "number 15\n",
      "idiomatic 7\n",
      "literal 8\n",
      "===\n",
      "Test\n",
      "number 15\n",
      "idiomatic 8\n",
      "literal 7\n",
      "===\n",
      "Extended Evaluation\n",
      "number 100\n",
      "idiomatic 46\n",
      "literal 54\n",
      "===\n"
     ]
    }
   ],
   "source": [
    "for sub in [\"Sample\", \"Train\", \"Dev\", \"Test\", \"Extended Evaluation\"]:\n",
    "    print(sub)\n",
    "    types = dataA[dataA[\"subset\"] == sub][\"sentence_type\"]\n",
    "    number_idiomatic = types[types == \"idiomatic\"].count()\n",
    "    number_literal = types[types == \"literal\"].count()\n",
    "    print(\"number\", len(types) )\n",
    "    print(\"ratio literal\", round(number_literal / len(types),2) ) \n",
    "    print(\"ratio idiomatic\", round(number_idiomatic / len(types),2) )\n",
    "    print(\"===\")\n",
    "\n",
    "for sub in [\"Sample\", \"Train\", \"Dev\", \"Test\", \"Extended Evaluation\"]:\n",
    "    print(sub)\n",
    "    types = dataA[dataA[\"subset\"] == sub][\"sentence_type\"]\n",
    "    number_idiomatic = types[types == \"idiomatic\"].count()\n",
    "    number_literal = types[types == \"literal\"].count()\n",
    "    print(\"number\", len(types) )\n",
    "    print(\"idiomatic\", number_idiomatic)\n",
    "    print(\"literal\", number_literal)\n",
    "    print(\"===\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute model embeddings (BERT-mode)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea: we use a pretrained BERT model to generate embeddings of sentences and of the compound in the context of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from transformers import AutoModel\n",
    "\n",
    "# model is selected from https://huggingface.co/models\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModel.from_pretrained(checkpoint, output_hidden_states=True).to(device)\n",
    "model = model.eval()\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch, padding=True, truncation=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Computing sentence-based embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_attention_tokens(attention_mask_item):\n",
    "    attention_mask_new = attention_mask_item.clone()\n",
    "    one_indices = (attention_mask_item == 1).nonzero(as_tuple=True)[0]\n",
    "    if remove_CLS_SEP == True:\n",
    "        # Setze die erste und letzte 1 auf 0: [CLS] und [SEP] Tokens\n",
    "        attention_mask_new[one_indices[0]] = 0  # Erste 1\n",
    "        attention_mask_new[one_indices[-1]] = 0  # Letzte 1\n",
    "    return attention_mask_new\n",
    "\n",
    "\n",
    "# different pooling methods for embeddings are computed\n",
    "\n",
    "def get_sentence_embedding(hidden_states,method,attention_mask):\n",
    "    sentence_embedding = []\n",
    "    if method == 'meanLast4': # average of all tokens of the last 4 layers\n",
    "        for i in range(len(hidden_states[0])):\n",
    "            # token_vecs is mean of last 4 layers\n",
    "            token_tensor = torch.stack([hidden_states[-1][i], hidden_states[-2][i], hidden_states[-3][i], hidden_states[-4][i]], dim=0)\n",
    "            token_vecs = torch.mean(token_tensor, dim=0)\n",
    "            attention = get_attention_tokens(attention_mask[i]) \n",
    "            token_vecs = token_vecs[attention.bool()]\n",
    "            sentence_embedding.append(torch.mean(token_vecs, dim=0))\n",
    "    elif method == 'meanSecondToLast': # average of second to last layer\n",
    "        for i in range(len(hidden_states[-2])):\n",
    "            token_vecs = hidden_states[-2][i]\n",
    "            attention = get_attention_tokens(attention_mask[i]) \n",
    "            token_vecs = token_vecs[attention.bool()]\n",
    "            sentence_embedding.append(torch.mean(token_vecs, dim=0))\n",
    "    elif method == 'meanLast': # average of last layer\n",
    "        for i in range(len(hidden_states[-1])):\n",
    "            token_vecs = hidden_states[-1][i]\n",
    "            attention = get_attention_tokens(attention_mask[i]) \n",
    "            token_vecs = token_vecs[attention.bool()]\n",
    "            sentence_embedding.append(torch.mean(token_vecs, dim=0))\n",
    "    elif method == 'lastCLS': # CLS token of last layer\n",
    "        sentence_embedding = hidden_states[-1][:, 0, :]\n",
    "    elif method == 'meanFirst': # average of first layer\n",
    "        for i in range(len(hidden_states[0])):\n",
    "            token_vecs = hidden_states[0][i]\n",
    "            attention = get_attention_tokens(attention_mask[i]) \n",
    "            token_vecs = token_vecs[attention.bool()]\n",
    "            sentence_embedding.append(torch.mean(token_vecs, dim=0))\n",
    "    elif method == 'firstCLS': # CLS token of first layer\n",
    "        sentence_embedding = hidden_states[0][:, 0, :]\n",
    "    return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence\n",
      "image1_caption\n",
      "image2_caption\n",
      "image3_caption\n",
      "image4_caption\n",
      "image5_caption\n",
      "gpt_idiomatic_meaning\n",
      "gpt_literal_meaning\n",
      "gpt_idiomatic_meaning_cutted\n",
      "gpt_literal_meaning_cutted\n",
      "gpt_idiomatic_sentence\n",
      "gpt_literal_sentence\n",
      "gpt_idiomatic_image\n",
      "gpt_literal_image\n"
     ]
    }
   ],
   "source": [
    "\n",
    "methods = ['meanSecondToLast','meanLast4','meanLast','meanFirst','firstCLS','lastCLS']\n",
    "\n",
    "for column in sentence_type_columns:\n",
    "    print(column)\n",
    "    dataA_sentence_tokenized = tokenize(dataA[column].tolist())\n",
    "    \n",
    "    # convert input_ids to tensor\n",
    "    input_ids_sentence = torch.tensor(dataA_sentence_tokenized[\"input_ids\"]).to(device)\n",
    "    attention_mask_sentence = torch.tensor(dataA_sentence_tokenized[\"attention_mask\"]).to(device)\n",
    "\n",
    "    # pass input_ids to model\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids_sentence, attention_mask_sentence)\n",
    "    \n",
    "    hidden_states_sentence = output.hidden_states\n",
    "\n",
    "    # use all methods for getting sentence embeddings and add them to dataA\n",
    "\n",
    "\n",
    "    for method in methods:\n",
    "        X = get_sentence_embedding(hidden_states_sentence,method,attention_mask_sentence)\n",
    "        X = np.array([x.cpu().numpy() for x in X]).tolist()\n",
    "        # add a new column to dataA \n",
    "        dataA[column + '_embedding_'+ method] = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 13   (including initial embeddings)\n",
      "Number of batches: 200\n",
      "Number of tokens: 20\n",
      "Number of hidden units: 768\n"
     ]
    }
   ],
   "source": [
    "print (\"Number of layers:\", len(hidden_states_sentence), \"  (including initial embeddings)\")\n",
    "layer_i = 0\n",
    "print (\"Number of batches:\", len(hidden_states_sentence[layer_i]))\n",
    "batch_i = 0\n",
    "print (\"Number of tokens:\", len(hidden_states_sentence[layer_i][batch_i]))\n",
    "token_i = 0\n",
    "print (\"Number of hidden units:\", len(hidden_states_sentence[layer_i][batch_i][token_i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing compound-based embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes the compound occurs in the sentence only in plural form. So both forms are needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "##!pip install inflect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sometimes the compound occurs in plural form in the sentence\n",
    "\n",
    "# add a new column to dataA with the plural form of the compound \n",
    "\n",
    "\n",
    "from re import *\n",
    "import inflect\n",
    "\n",
    "engine = inflect.engine()\n",
    "\n",
    "dataA[\"compound_plural\"] = [None for i in range(len(dataA))]\n",
    "\n",
    "for i in range(len(dataA[\"compound\"])):\n",
    "    dataA[\"compound_plural\"][i] = engine.plural(dataA[\"compound\"][i])\n",
    "\n",
    "# tokenize all compounds (original and plural)\n",
    "dataA_compound_tokenized = tokenize(dataA[\"compound\"].tolist())\n",
    "dataA_compound_plural_tokenized = tokenize(dataA[\"compound_plural\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the index of the compound in the sentence\n",
    "def get_idx(compound_tokens, compound_plural_tokens, sentence_tokens):\n",
    "    # remove 0-tokens from compound_tokens (removes tokens that are due to padding)\n",
    "    compound_tokens = [i for i in compound_tokens if i != 0]\n",
    "    # remove [CLS] and [SEP] from compound_tokens\n",
    "    compound_tokens = compound_tokens[1:-1]\n",
    "    compound_plural_tokens = [i for i in compound_plural_tokens if i != 0]\n",
    "    compound_plural_tokens = compound_plural_tokens[1:-1]\n",
    "    idx = []\n",
    "    # find the first occurence of the sequence of compound_tokens in sentence_tokens (singular and plural forms)\n",
    "    for i in range(len(sentence_tokens)):\n",
    "        if sentence_tokens[i:i+len(compound_tokens)] == compound_tokens:\n",
    "            for j in range(i, i+ len(compound_tokens)):\n",
    "                idx.append(j)\n",
    "    for i in range(len(sentence_tokens)):\n",
    "        if sentence_tokens[i:i+len(compound_plural_tokens)] == compound_plural_tokens:\n",
    "            for j in range(i, i+ len(compound_plural_tokens)):\n",
    "                idx.append(j)\n",
    "    # remove duplicates from idx\n",
    "    idx = list(set(idx))\n",
    "    return idx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the embeddings of the tokens in idxList. \n",
    "# The embeddings are combined to a single embedding by different averaging methods\n",
    "import numpy as np\n",
    "def get_idxList_embedding(hidden_states,idxLists,method):\n",
    "    embedding = []\n",
    "    if method == 'meanLast4':\n",
    "        for i in range(len(hidden_states[-1])):\n",
    "            # token_vecs is mean of last 4 layers\n",
    "            idxList = idxLists[i]\n",
    "            token_tensor = torch.stack([hidden_states[-1][i][idxList], hidden_states[-2][i][idxList], hidden_states[-3][i][idxList], hidden_states[-4][i][idxList]], dim=0)\n",
    "            token_vecs = torch.mean(token_tensor, dim=0)\n",
    "            embedding.append(torch.mean(token_vecs, dim=0).tolist())\n",
    "    elif method == 'meanSecondToLast':\n",
    "        for i in range(len(hidden_states[-2])):\n",
    "            idxList = idxLists[i]\n",
    "            token_vecs = hidden_states[-2][i][idxList]\n",
    "            embedding.append(torch.mean(token_vecs, dim=0).tolist())\n",
    "    elif method == 'meanLast':\n",
    "        for i in range(len(hidden_states[-1])):\n",
    "            idxList = idxLists[i]\n",
    "            token_vecs = hidden_states[-1][i][idxList]\n",
    "            embedding.append(torch.mean(token_vecs, dim=0).tolist())\n",
    "    elif method == 'meanFirst':\n",
    "        for i in range(len(hidden_states[0])):\n",
    "            idxList = idxLists[i]\n",
    "            token_vecs = hidden_states[0][i][idxList]\n",
    "            embedding.append(torch.mean(token_vecs, dim=0).tolist())\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence embeddings \n",
    "\n",
    "compound_methods = ['meanSecondToLast','meanLast4','meanLast','meanFirst']\n",
    "\n",
    "compound_tokenized = tokenize(dataA[\"compound\"].tolist())\n",
    "compound_plural_tokenized = tokenize(dataA[\"compound_plural\"].tolist())    \n",
    "\n",
    "for column in sentence_with_compound_columns:\n",
    "    # tokenize the column\n",
    "    tokenized = tokenize(dataA[column].tolist())\n",
    "\n",
    "    # hidden states for gpt_Meaning\n",
    "    input_ids = torch.tensor(tokenized[\"input_ids\"]).to(device)\n",
    "    attention_mask = torch.tensor(tokenized[\"attention_mask\"]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids, attention_mask)\n",
    "\n",
    "    hidden_states = output.hidden_states\n",
    " \n",
    "    # add gpt_compound_embeddings\n",
    " \n",
    "    # add column to dataA with the indices of the compound in the sentence\n",
    "    dataA[column + \"_compound_idx\"] = [get_idx(compound_tokenized[\"input_ids\"][i], \n",
    "                                               compound_plural_tokenized[\"input_ids\"][i], \n",
    "                                               tokenized[\"input_ids\"][i]) for i in range(len(dataA))]\n",
    "    \n",
    "    # apply the methods in compound_methods to get the embeddings of the compound\n",
    "    for method in compound_methods:\n",
    "        dataA['compound_embedding_'+ column + \"_\"+ method] = get_idxList_embedding(hidden_states,\n",
    "                                                                                  dataA[column + \"_compound_idx\"],\n",
    "                                                                                  method) \n",
    "    dataA = dataA.copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence\n",
      "white hat\n",
      "use white ethical search engine optimization technique lead long term success many seo firm use tactic risk harming site\n",
      "58\n",
      "sentence\n",
      "pig ear\n",
      "wrote new partition table made right pig\n",
      "152\n",
      "sentence\n",
      "pea pod\n",
      "spearfishermen shape size pod gun optimize design purpose\n",
      "183\n",
      "gpt_literal_sentence\n",
      "loan shark\n",
      "aquarium exhibit featured model shark alongside fact marine life\n",
      "100\n",
      "gpt_literal_sentence\n",
      "loan shark\n",
      "aquarium exhibit featured model shark alongside fact marine life\n",
      "101\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-0.012831419706344604, -1.065843105316162, 0.3485209345817566]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print  if compound_idx is empty (ideally there should be no empty compound_idx)\n",
    "for column in sentence_with_compound_columns:\n",
    "    for i in range(len(dataA)):\n",
    "        if len(dataA[column + \"_compound_idx\"][i]) == 0:\n",
    "            print(column)\n",
    "            print(dataA[\"compound\"][i])\n",
    "            print(dataA[column][i])\n",
    "            print(i)\n",
    "            \n",
    "dataA[\"compound_embedding_gpt_literal_sentence_meanLast\"][91][:3]\n",
    "# if idx = [] then embedding = nan's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace nan-embeddings (due to missing compound) by corresponding sentence embedding:\n",
    "for column in [\"gpt_literal_sentence\", \"gpt_idiomatic_sentence\", \"gpt_literal_meaning\", \n",
    "               \"gpt_idiomatic_meaning\",\"sentence\" ]:\n",
    "    for method in compound_methods:\n",
    "        for i in range(len(dataA)):\n",
    "            if len(dataA[column + \"_compound_idx\"][i]) == 0:\n",
    "                dataA['compound_embedding_'+ column + \"_\"+ method][i] = dataA[column + \"_embedding_\"+ method][i]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate sBERT embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U sentence-transformers\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  SBert embeddings are generated  for all sentence like columns\n",
    "for type in sentence_type_columns:\n",
    "    dataA[type + \"_embedding_sbert\"] = dataA[type].apply(lambda x: model.encode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writes all column names in a file for later reference\n",
    "with open('column.txt', 'w') as f:\n",
    "    for c in dataA.columns:\n",
    "        print(c,  file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save dataA pickle\n",
    "import pickle\n",
    "if preprocessed == True:\n",
    "    prep = \"_preprocessed_\"\n",
    "else:\n",
    "    prep = \"_\"\n",
    "\n",
    "if remove_CLS_SEP == True:\n",
    "    cls_sep = \"_without_CLS_SEP\"\n",
    "else:\n",
    "    cls_sep = \"\"\n",
    "\n",
    "checkpoint_write = checkpoint.replace(\"/\", \"_\").replace(\"\\\\\", \"_\")\n",
    "dataA.to_pickle(\"dataA\"+ prep + checkpoint_write + cls_sep + \".pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
