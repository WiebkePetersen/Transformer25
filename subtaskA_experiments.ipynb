{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions\n",
    "\n",
    "Needs pkl-data provided by notebook subtaskA_compute_embeddings.ipynb.\n",
    "\n",
    "## Load data and investigate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "import ast\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Parameter setting\n",
    "\n",
    "# model\n",
    "checkpoint_ptlm = 'bert-base-uncased' \n",
    "checkpoint_ftlm ='jlsalim/bert-uncased-idiomatic-literal-recognizer'\n",
    "\n",
    "checkpoint = checkpoint_ptlm\n",
    "\n",
    "# full sentences or preprocessed (filtered to content words of specific POS)\n",
    "preprocessed = False \n",
    "#preprocessed = True\n",
    "\n",
    "#remove_CLS_SEP = True \n",
    "remove_CLS_SEP = False\n",
    "\n",
    "\n",
    "# load pkl-file\n",
    "import pickle\n",
    "def load_pkl(preprocessed, checkpoint, remove_CLS_SEP):\n",
    "    if preprocessed == True:\n",
    "        prep = \"_preprocessed_\"\n",
    "    else:\n",
    "        prep = \"_\"\n",
    "\n",
    "    if remove_CLS_SEP == True:\n",
    "        cls_sep = \"_without_CLS_SEP\"\n",
    "    else:\n",
    "        cls_sep = \"\"\n",
    "\n",
    "    checkpoint_write = checkpoint.replace(\"/\", \"_\").replace(\"\\\\\", \"_\")\n",
    "    dataA = pd.read_pickle(\"dataA\"+ prep + checkpoint_write + cls_sep + \".pkl\")\n",
    "    # some data cleaning \n",
    "    # if expected order is string, convert to list split by comma\n",
    "    for i in range(len(dataA[\"expected_order\"])):\n",
    "        if isinstance(dataA[\"expected_order\"][i], str):\n",
    "            dataA[\"expected_order\"][i] = ast.literal_eval(dataA[\"expected_order\"][i])\n",
    "\n",
    "    return dataA\n",
    "\n",
    "dataA = load_pkl(preprocessed, checkpoint, remove_CLS_SEP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Extended Evaluation    100\n",
       "Train                   60\n",
       "Dev                     15\n",
       "Test                    15\n",
       "Sample                  10\n",
       "Name: subset, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataA[\"subset\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "idiomatic    100\n",
       "literal      100\n",
       "Name: sentence_type, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataA['sentence_type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data handling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the dataframe sample and train (data items that have literal/idiomatic information given)\n",
    "def only_train(dataA): # returns the dataframe sample and train (data items that have literal/idiomatic information given)\n",
    "    return pd.concat([dataA[dataA[\"subset\"] == \"Sample\"],dataA[dataA[\"subset\"]== \"Train\"]])\n",
    "\n",
    "# returns the dataframe of subset\n",
    "def only_subset(dataA, subset): # returns the dataframe sample and train (data items that have literal/idiomatic information given)\n",
    "    return dataA[dataA[\"subset\"] == subset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generates submission file from column for subset\n",
    "from zipfile import ZipFile\n",
    "def make_submission(dataA, column, subset):\n",
    "    if subset == \"Dev\":\n",
    "        name = \"EN\"\n",
    "    if subset == \"Extended Evaluation\":\n",
    "        name = \"xe\"\n",
    "    if subset == \"Test\":\n",
    "        name = \"EN\"\n",
    "    full_name = \"submission_\" + name\n",
    "    subset_data = only_subset(dataA,subset)\n",
    "    submission_df = pd.DataFrame()\n",
    "    submission_df[\"compound\"] = subset_data[\"compound\"]\n",
    "    submission_df[\"expected_order\"] = subset_data[column]\n",
    "    submission_df.to_csv(full_name + \".tsv\", sep=\"\\t\", index=False)\n",
    "    ZipFile(full_name + '.zip', 'w').write(full_name + '.tsv')\n",
    "    print(\"File zipped and saved as \" +  full_name + \".zip\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to display images\n",
    "\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "fileDirectory = 'D:\\\\Wiebke Petersen\\\\Downloads\\\\AdMIRe Subtask A Train\\\\train' # set to your directory with the images\n",
    "\n",
    "# Open the image file\n",
    "def display_image(compound, fn):\n",
    "    img = Image.open(fileDirectory + \"\\\\\" + compound + \"\\\\\" + fn)\n",
    "    new_size = (150, 150)  # Width, Height\n",
    "    img_resized = img.resize(new_size)  \n",
    "    # Display the image\n",
    "    display(img_resized)\n",
    "\n",
    "# returns list of image names sorted from image1 to image5\n",
    "def get_image_names(n,mydata):\n",
    "    names = []\n",
    "    for i in [1,2,3,4,5]:\n",
    "         names.append(mydata['image' + str(i) + '_name'][n])\n",
    "    return names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print information of 1 item:\n",
    "\n",
    "def print_item(n, mydata):\n",
    "    # print  'sentence_type', 'sentence'\n",
    "    compound = mydata['compound'][n]\n",
    "    print(compound)\n",
    "    print(mydata['sentence_type'][n])\n",
    "    print(mydata['sentence'][n])\n",
    "    print('---------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "    # for image_names in 'expected_order' print image_captions\n",
    "    names  =  get_image_names(n,mydata)\n",
    "    expected_order = mydata['expected_order'][n]\n",
    "    print(expected_order)\n",
    "\n",
    "    for image_name in expected_order:\n",
    "        display_image(compound, image_name)\n",
    "        # get index of image_name in names\n",
    "        index = names.index(image_name) + 1\n",
    "        print(mydata['image'+str(index)+'_caption'][n])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataA_train = only_train(dataA)\n",
    "#print_item(10,dataA_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# evaluation functions for ranked orders\n",
    "def top1accuracy(pred_rankings,expected_order):\n",
    "    pred_rankings = pred_rankings.to_list()\n",
    "    expected_order = expected_order.to_list()\n",
    "    correct = 0\n",
    "    for i in range(len(pred_rankings)):\n",
    "        if pred_rankings[i][0] == expected_order[i][0]:\n",
    "            correct += 1\n",
    "    return round(correct/len(pred_rankings),4)\n",
    "\n",
    "\n",
    "def spearman_correlation(pred_rankings,expected_order):\n",
    "    pred_rankings = pred_rankings.to_list()\n",
    "    expected_order = expected_order.to_list()\n",
    "    corr = []\n",
    "    for i in range(len(pred_rankings)):\n",
    "        corr.append(spearmanr(pred_rankings[i],expected_order[i]).correlation)\n",
    "    return round(np.mean(corr),4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simlarity functions\n",
    "sim_function = \"cosine_similarity\" # \"manhattan\" \"cosine_similarity\"\n",
    "#sim_function = \"manhattan\" # \"cosine_similarity\"\n",
    "\n",
    "\n",
    "def cosine_similarity(u, v):\n",
    "    u = np.array(u)\n",
    "    v = np.array(v)\n",
    "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))\n",
    "\n",
    "def manhattan(u, v):\n",
    "    u = np.array(u)\n",
    "    v = np.array(v)\n",
    "#    return 1/(np.sum(np.abs(u - v))+1)\n",
    "    return -np.sum(np.abs(u - v))\n",
    "\n",
    "# similarity of two embeddings\n",
    "def similarity(u,v):\n",
    "    if sim_function == \"cosine_similarity\":\n",
    "        return cosine_similarity(u,v)\n",
    "    elif sim_function == \"manhattan\":\n",
    "        return manhattan(u,v)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown similarity function: {}\".format(sim_function))\n",
    "\n",
    "\n",
    "# compare two embeddings to a comparator embedding and return the index of the more similar one\n",
    "def compare(emb,emb0,emb1):\n",
    "    sim0 = similarity(emb,emb0)\n",
    "    sim1 = similarity(emb,emb1)\n",
    "    if sim0 > sim1:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def binary2values(list,val0,val1):\n",
    "    final = []\n",
    "    for element in list:\n",
    "        if element == 1:\n",
    "            final.append(val1)\n",
    "        else:\n",
    "            final.append(val0)\n",
    "    return final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fundction that compares the embeddings of the image captions to the embedding of the comparator and returns\n",
    "# the similarity scores for each image as a dictionary\n",
    "# the keys are the image names and the values are the similarity scores\n",
    "def sim_scores(current, comparator,method):\n",
    "    # input = current line(example) & embeddings for sentence + captions\n",
    "    \n",
    "    scores = {}\n",
    "    # keys = image names\n",
    "    # values = scores\n",
    "    embeddings = [current[comparator+ \"_embedding_\" + method], \n",
    "                 current[\"image1_caption_embedding_\" + method],\n",
    "                 current[\"image2_caption_embedding_\" + method],\n",
    "                 current[\"image3_caption_embedding_\" + method],\n",
    "                 current[\"image4_caption_embedding_\" + method],\n",
    "                 current[\"image5_caption_embedding_\" + method]]\n",
    "\n",
    "    similarities = [similarity(embeddings[0], embeddings[i]) for i in range(len(embeddings))]\n",
    "    # compares the embedding for the sentence including the compound \n",
    "    # with each of the embeddings, including itself and all the captions\n",
    "\n",
    "    score1 = similarities[1].item()\n",
    "    scores[current[\"image1_name\"]] = score1\n",
    "\n",
    "    score2 = similarities[2].item()\n",
    "    scores[current[\"image2_name\"]] = score2\n",
    "\n",
    "    score3 = similarities[3].item()\n",
    "    scores[current[\"image3_name\"]] = score3\n",
    "\n",
    "    score4 = similarities[4].item()\n",
    "    scores[current[\"image4_name\"]] = score4\n",
    "\n",
    "    score5 = similarities[5].item()\n",
    "    scores[current[\"image5_name\"]] = score5\n",
    "    \n",
    "    return scores\n",
    "\n",
    "\n",
    "# fundction that compares the embeddings of the image captions to the embedding of the compound in the \n",
    "# comparator and returns\n",
    "# the similarity scores for each image as a dictionary\n",
    "# the keys are the image names and the values are the similarity scores\n",
    "def sim_scores_compound(current, comparator,method):\n",
    "    # input = current line(example) & embeddings for sentence + captions\n",
    "    \n",
    "    scores = {}\n",
    "    # keys = image names\n",
    "    # values = scores\n",
    "    embeddings = [current[\"compound_embedding_\" + comparator + \"_\" +  method], \n",
    "                 current[\"image1_caption_embedding_\" + method],\n",
    "                 current[\"image2_caption_embedding_\" + method],\n",
    "                 current[\"image3_caption_embedding_\" + method],\n",
    "                 current[\"image4_caption_embedding_\" + method],\n",
    "                 current[\"image5_caption_embedding_\" + method]]\n",
    "\n",
    "    similarities = [similarity(embeddings[0], embeddings[i]) for i in range(len(embeddings))]\n",
    "    # compares the embedding for the sentence including the compound \n",
    "    # with each of the embeddings, including itself and all the captions\n",
    "\n",
    "    score1 = similarities[1].item()\n",
    "    scores[current[\"image1_name\"]] = score1\n",
    "\n",
    "    score2 = similarities[2].item()\n",
    "    scores[current[\"image2_name\"]] = score2\n",
    "\n",
    "    score3 = similarities[3].item()\n",
    "    scores[current[\"image3_name\"]] = score3\n",
    "\n",
    "    score4 = similarities[4].item()\n",
    "    scores[current[\"image4_name\"]] = score4\n",
    "\n",
    "    score5 = similarities[5].item()\n",
    "    scores[current[\"image5_name\"]] = score5\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic ranking by similarity scores (most similar first)\n",
    "# used as baseline ranker in paper \n",
    "# returns a list of image names sorted by similarity scores (most similar first)\n",
    "def rank_images(scores):\n",
    "    # scores = dictionary containing the cos similarity scores\n",
    "    # comparing the captions of the five images with some comparator embedding\n",
    "    # keys = image names\n",
    "    # values = scores\n",
    "\n",
    "    ranking = []\n",
    "\n",
    "    for i in range(5):\n",
    "        # find key which corresponds to the highest value\n",
    "        m = max(scores, key=scores.get)\n",
    "        # add the key (image name) to the ranking\n",
    "        ranking.append(m)\n",
    "        # delete the entry in the dictionary\n",
    "        del scores[m]\n",
    "\n",
    "    return ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1 (only given data, no GPT-generated additional data used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed = False \n",
    "dataA = load_pkl(preprocessed, checkpoint_ptlm, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank images by similarity to original sentence:\n",
      "Method:  meanSecondToLast on train\n",
      "top1 0.3714  spearman 0.0686\n",
      "Method:  meanLast4 on train\n",
      "top1 0.3571  spearman 0.0971\n",
      "Method:  meanLast on train\n",
      "top1 0.4  spearman 0.0071\n",
      "Method:  lastCLS on train\n",
      "top1 0.2143  spearman -0.0129\n",
      "Method:  sbert on train\n",
      "top1 0.4  spearman 0.2014\n",
      "\n",
      "Rank images by similarity to compound embedding:\n",
      "Method:  meanSecondToLast on train\n",
      "top1 0.2571  spearman 0.0871\n",
      "Method:  meanLast4 on train\n",
      "top1 0.2286  spearman 0.0743\n",
      "Method:  meanLast on train\n",
      "top1 0.2571  spearman 0.0314\n"
     ]
    }
   ],
   "source": [
    "comparator = \"sentence\"\n",
    "\n",
    "methods = [\n",
    "    'meanSecondToLast',\n",
    "    'meanLast4',\n",
    "    'meanLast',\n",
    "    #'meanFirst',\n",
    "    #'firstCLS',\n",
    "    'lastCLS',\n",
    "    'sbert']\n",
    "subset = \"train\"\n",
    "\n",
    "print(\"Rank images by similarity to original sentence:\")\n",
    "for method in methods:\n",
    "    dataA[\"pred_order\"] = dataA.apply(lambda x: rank_images(sim_scores(x, comparator, method)), axis=1)\n",
    "    if subset == \"train\":\n",
    "        mydata = only_train(dataA)\n",
    "    else:\n",
    "        mydata = only_subset(dataA,subset)\n",
    "\n",
    "    print(\"Method: \", method, \"on\", subset)\n",
    "    print(\"top1\", top1accuracy(mydata[\"expected_order\"], mydata[\"pred_order\"]), \" spearman\", spearman_correlation(mydata[\"expected_order\"], mydata[\"pred_order\"]))\n",
    "\n",
    "methods_compound = [\n",
    "    'meanSecondToLast',\n",
    "    'meanLast4',\n",
    "    'meanLast',\n",
    "    #'meanFirst'\n",
    "    ]\n",
    "\n",
    "# compare to compound embedding\n",
    "print(\"\\nRank images by similarity to compound embedding:\")\n",
    "for method in methods_compound:\n",
    "    dataA[\"pred_order\"] = dataA.apply(lambda x: rank_images(sim_scores_compound(x, comparator, method)), axis=1)\n",
    "    if subset == \"train\":\n",
    "        mydata = only_train(dataA)\n",
    "    else:\n",
    "        mydata = only_subset(dataA,subset)\n",
    "\n",
    "    print(\"Method: \", method, \"on\", subset)\n",
    "    print(\"top1\", top1accuracy(mydata[\"expected_order\"], mydata[\"pred_order\"]), \" spearman\", spearman_correlation(mydata[\"expected_order\"], mydata[\"pred_order\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "only on literal sentences\n",
      "Method:  meanSecondToLast on train\n",
      "top1 0.6129  spearman 0.1968\n",
      "Method:  meanLast4 on train\n",
      "top1 0.5484  spearman 0.1903\n",
      "Method:  meanLast on train\n",
      "top1 0.5484  spearman 0.1452\n",
      "Method:  lastCLS on train\n",
      "top1 0.2903  spearman 0.1161\n",
      "Method:  sbert on train\n",
      "top1 0.6774  spearman 0.3065\n",
      "\n",
      "only on idiomatic sentences\n",
      "Method:  meanSecondToLast on train\n",
      "top1 0.1795  spearman -0.0333\n",
      "Method:  meanLast4 on train\n",
      "top1 0.2051  spearman 0.0231\n",
      "Method:  meanLast on train\n",
      "top1 0.2821  spearman -0.1026\n",
      "Method:  lastCLS on train\n",
      "top1 0.1538  spearman -0.1154\n",
      "Method:  sbert on train\n",
      "top1 0.1795  spearman 0.1179\n"
     ]
    }
   ],
   "source": [
    "print(\"only on literal sentences\")\n",
    "for method in methods:\n",
    "    dataA[\"pred_order\"] = dataA.apply(lambda x: rank_images(sim_scores(x, comparator, method)), axis=1)\n",
    "    if subset == \"train\":\n",
    "        mydata = only_train(dataA)\n",
    "    else:\n",
    "        mydata = only_subset(dataA,subset)\n",
    "    mydata = mydata[mydata[\"sentence_type\"] == \"literal\"]\n",
    "    print(\"Method: \", method, \"on\", subset)\n",
    "    print(\"top1\", top1accuracy(mydata[\"expected_order\"], mydata[\"pred_order\"]), \" spearman\", spearman_correlation(mydata[\"expected_order\"], mydata[\"pred_order\"]))\n",
    "\n",
    "print(\"\\nonly on idiomatic sentences\")\n",
    "for method in methods:\n",
    "    dataA[\"pred_order\"] = dataA.apply(lambda x: rank_images(sim_scores(x, comparator, method)), axis=1)\n",
    "    if subset == \"train\":\n",
    "        mydata = only_train(dataA)\n",
    "    else:\n",
    "        mydata = only_subset(dataA,subset)\n",
    "    mydata = mydata[mydata[\"sentence_type\"] == \"idiomatic\"]\n",
    "    print(\"Method: \", method, \"on\", subset)\n",
    "    print(\"top1\", top1accuracy(mydata[\"expected_order\"], mydata[\"pred_order\"]), \" spearman\", spearman_correlation(mydata[\"expected_order\"], mydata[\"pred_order\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compound embedding only on literal sentences\n",
      "Method:  meanSecondToLast on train\n",
      "top1 0.5161  spearman 0.1774\n",
      "Method:  meanLast4 on train\n",
      "top1 0.4839  spearman 0.2387\n",
      "Method:  meanLast on train\n",
      "top1 0.4839  spearman 0.2097\n",
      "\n",
      "compound embedding only on idiomatic sentences\n",
      "Method:  meanSecondToLast on train\n",
      "top1 0.0513  spearman 0.0154\n",
      "Method:  meanLast4 on train\n",
      "top1 0.0256  spearman -0.0564\n",
      "Method:  meanLast on train\n",
      "top1 0.0769  spearman -0.1103\n"
     ]
    }
   ],
   "source": [
    "print(\"compound embedding only on literal sentences\")\n",
    "for method in methods_compound:\n",
    "    dataA[\"pred_order\"] = dataA.apply(lambda x: rank_images(sim_scores_compound(x, comparator, method)), axis=1)\n",
    "    if subset == \"train\":\n",
    "        mydata = only_train(dataA)\n",
    "    else:\n",
    "        mydata = only_subset(dataA,subset)\n",
    "    mydata = mydata[mydata[\"sentence_type\"] == \"literal\"]\n",
    "    print(\"Method: \", method, \"on\", subset)\n",
    "    print(\"top1\", top1accuracy(mydata[\"expected_order\"], mydata[\"pred_order\"]), \" spearman\", spearman_correlation(mydata[\"expected_order\"], mydata[\"pred_order\"]))\n",
    "\n",
    "print(\"\\ncompound embedding only on idiomatic sentences\")\n",
    "for method in methods_compound:\n",
    "    dataA[\"pred_order\"] = dataA.apply(lambda x: rank_images(sim_scores_compound(x, comparator, method)), axis=1)\n",
    "    if subset == \"train\":\n",
    "        mydata = only_train(dataA)\n",
    "    else:\n",
    "        mydata = only_subset(dataA,subset)\n",
    "    mydata = mydata[mydata[\"sentence_type\"] == \"idiomatic\"]\n",
    "    print(\"Method: \", method, \"on\", subset)\n",
    "    print(\"top1\", top1accuracy(mydata[\"expected_order\"], mydata[\"pred_order\"]), \" spearman\", spearman_correlation(mydata[\"expected_order\"], mydata[\"pred_order\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**with preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed = True\n",
    "dataA = load_pkl(preprocessed, checkpoint_ptlm, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank images by similarity to original sentence:\n",
      "Method:  meanSecondToLast on train\n",
      "top1 0.3286  spearman 0.0771\n",
      "Method:  meanLast4 on train\n",
      "top1 0.3143  spearman 0.0943\n",
      "Method:  meanLast on train\n",
      "top1 0.3571  spearman 0.0971\n",
      "Method:  lastCLS on train\n",
      "top1 0.2714  spearman 0.0543\n",
      "Method:  sbert on train\n",
      "top1 0.4571  spearman 0.2057\n",
      "\n",
      "Rank images by similarity to compound embedding:\n",
      "Method:  meanSecondToLast on train\n",
      "top1 0.3143  spearman 0.1729\n",
      "Method:  meanLast4 on train\n",
      "top1 0.3143  spearman 0.1114\n",
      "Method:  meanLast on train\n",
      "top1 0.2857  spearman 0.0857\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Rank images by similarity to original sentence:\")\n",
    "for method in methods:\n",
    "    dataA[\"pred_order\"] = dataA.apply(lambda x: rank_images(sim_scores(x, comparator, method)), axis=1)\n",
    "    if subset == \"train\":\n",
    "        mydata = only_train(dataA)\n",
    "    else:\n",
    "        mydata = only_subset(dataA,subset)\n",
    "\n",
    "    print(\"Method: \", method, \"on\", subset)\n",
    "    print(\"top1\", top1accuracy(mydata[\"expected_order\"], mydata[\"pred_order\"]), \" spearman\", spearman_correlation(mydata[\"expected_order\"], mydata[\"pred_order\"]))\n",
    "\n",
    "methods_compound = [\n",
    "    'meanSecondToLast',\n",
    "    'meanLast4',\n",
    "    'meanLast',\n",
    "    #'meanFirst'\n",
    "    ]\n",
    "\n",
    "# compare to compound embedding\n",
    "print(\"\\nRank images by similarity to compound embedding:\")\n",
    "for method in methods_compound:\n",
    "    dataA[\"pred_order\"] = dataA.apply(lambda x: rank_images(sim_scores_compound(x, comparator, method)), axis=1)\n",
    "    if subset == \"train\":\n",
    "        mydata = only_train(dataA)\n",
    "    else:\n",
    "        mydata = only_subset(dataA,subset)\n",
    "\n",
    "    print(\"Method: \", method, \"on\", subset)\n",
    "    print(\"top1\", top1accuracy(mydata[\"expected_order\"], mydata[\"pred_order\"]), \" spearman\", spearman_correlation(mydata[\"expected_order\"], mydata[\"pred_order\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "only on idiomatic sentences\n",
      "Method:  meanSecondToLast on train\n",
      "top1 0.2051  spearman 0.1026\n",
      "Method:  meanLast4 on train\n",
      "top1 0.2051  spearman 0.1051\n",
      "Method:  meanLast on train\n",
      "top1 0.2564  spearman 0.0538\n",
      "Method:  lastCLS on train\n",
      "top1 0.1795  spearman 0.0205\n",
      "Method:  sbert on train\n",
      "top1 0.2051  spearman 0.1333\n",
      "\n",
      "compound embedding only on idiomatic sentences\n",
      "Method:  meanSecondToLast on train\n",
      "top1 0.1026  spearman 0.0949\n",
      "Method:  meanLast4 on train\n",
      "top1 0.1026  spearman 0.0359\n",
      "Method:  meanLast on train\n",
      "top1 0.1282  spearman -0.0487\n"
     ]
    }
   ],
   "source": [
    "# only on idiomatic sentences\n",
    "print(\"\\nonly on idiomatic sentences\")\n",
    "for method in methods:\n",
    "    dataA[\"pred_order\"] = dataA.apply(lambda x: rank_images(sim_scores(x, comparator, method)), axis=1)\n",
    "    if subset == \"train\":\n",
    "        mydata = only_train(dataA)\n",
    "    else:\n",
    "        mydata = only_subset(dataA,subset)\n",
    "    mydata = mydata[mydata[\"sentence_type\"] == \"idiomatic\"]\n",
    "    print(\"Method: \", method, \"on\", subset)\n",
    "    print(\"top1\", top1accuracy(mydata[\"expected_order\"], mydata[\"pred_order\"]), \" spearman\", spearman_correlation(mydata[\"expected_order\"], mydata[\"pred_order\"]))\n",
    "\n",
    "# compound embedding only on idiomatic sentences\n",
    "print(\"\\ncompound embedding only on idiomatic sentences\")\n",
    "for method in methods_compound:\n",
    "    dataA[\"pred_order\"] = dataA.apply(lambda x: rank_images(sim_scores_compound(x, comparator, method)), axis=1)\n",
    "    if subset == \"train\":\n",
    "        mydata = only_train(dataA)\n",
    "    else:\n",
    "        mydata = only_subset(dataA,subset)\n",
    "    mydata = mydata[mydata[\"sentence_type\"] == \"idiomatic\"]\n",
    "    print(\"Method: \", method, \"on\", subset)\n",
    "    print(\"top1\", top1accuracy(mydata[\"expected_order\"], mydata[\"pred_order\"]), \" spearman\", spearman_correlation(mydata[\"expected_order\"], mydata[\"pred_order\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "only on literal sentences\n",
      "Method:  meanSecondToLast on train\n",
      "top1 0.4839  spearman 0.0452\n",
      "Method:  meanLast4 on train\n",
      "top1 0.4516  spearman 0.0806\n",
      "Method:  meanLast on train\n",
      "top1 0.4839  spearman 0.1516\n",
      "Method:  lastCLS on train\n",
      "top1 0.3871  spearman 0.0968\n",
      "Method:  sbert on train\n",
      "top1 0.7742  spearman 0.2968\n",
      "\n",
      "compound embedding only on literal sentences\n",
      "Method:  meanSecondToLast on train\n",
      "top1 0.5806  spearman 0.271\n",
      "Method:  meanLast4 on train\n",
      "top1 0.5806  spearman 0.2065\n",
      "Method:  meanLast on train\n",
      "top1 0.4839  spearman 0.2548\n"
     ]
    }
   ],
   "source": [
    "# only on literal sentences\n",
    "print(\"\\nonly on literal sentences\")\n",
    "for method in methods:\n",
    "    dataA[\"pred_order\"] = dataA.apply(lambda x: rank_images(sim_scores(x, comparator, method)), axis=1)\n",
    "    if subset == \"train\":\n",
    "        mydata = only_train(dataA)\n",
    "    else:\n",
    "        mydata = only_subset(dataA,subset)\n",
    "    mydata = mydata[mydata[\"sentence_type\"] == \"literal\"]\n",
    "    print(\"Method: \", method, \"on\", subset)\n",
    "    print(\"top1\", top1accuracy(mydata[\"expected_order\"], mydata[\"pred_order\"]), \" spearman\", spearman_correlation(mydata[\"expected_order\"], mydata[\"pred_order\"]))\n",
    "\n",
    "# compound embedding only on literal sentences\n",
    "print(\"\\ncompound embedding only on literal sentences\")\n",
    "for method in methods_compound:\n",
    "    dataA[\"pred_order\"] = dataA.apply(lambda x: rank_images(sim_scores_compound(x, comparator, method)), axis=1)\n",
    "    if subset == \"train\":\n",
    "        mydata = only_train(dataA)\n",
    "    else:\n",
    "        mydata = only_subset(dataA,subset)\n",
    "    mydata = mydata[mydata[\"sentence_type\"] == \"literal\"]\n",
    "    print(\"Method: \", method, \"on\", subset)\n",
    "    print(\"top1\", top1accuracy(mydata[\"expected_order\"], mydata[\"pred_order\"]), \" spearman\", spearman_correlation(mydata[\"expected_order\"], mydata[\"pred_order\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2 (using GPT data and idiomatic/literal gold label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed = True\n",
    "dataA = load_pkl(preprocessed, checkpoint_ptlm, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank images by similarity (based on gold label):\n",
      "\n",
      " image\n",
      "Method:  sbert on train\n",
      "top1 0.6143  spearman 0.1586\n",
      "Method:  meanLast on train\n",
      "top1 0.4857  spearman 0.1371\n",
      "\n",
      " meaning\n",
      "Method:  sbert on train\n",
      "top1 0.4429  spearman 0.2914\n",
      "Method:  meanLast on train\n",
      "top1 0.4714  spearman 0.1743\n",
      "\n",
      " meaning_cutted\n",
      "Method:  sbert on train\n",
      "top1 0.5286  spearman 0.1857\n",
      "Method:  meanLast on train\n",
      "top1 0.5143  spearman 0.1257\n",
      "\n",
      " sentence\n",
      "Method:  sbert on train\n",
      "top1 0.3857  spearman 0.1471\n",
      "Method:  meanLast on train\n",
      "top1 0.3571  spearman 0.1386\n",
      "\n",
      "Rank images by similarity to compound embedding:\n",
      "\n",
      " meaning\n",
      "Method:  meanLast4 on train\n",
      "top1 0.3429  spearman 0.0743\n",
      "\n",
      " sentence\n",
      "Method:  meanLast4 on train\n",
      "top1 0.2571  spearman 0.1329\n"
     ]
    }
   ],
   "source": [
    "def get_comparator_prefix(sentence_type, gpt_type):\n",
    "    return \"gpt_\" + sentence_type + \"_\" + gpt_type\n",
    "\n",
    "gpt_types = [\"image\",\"meaning\",\"meaning_cutted\",\"sentence\"]\n",
    "\n",
    "print(\"Rank images by similarity (based on gold label):\")\n",
    "for gpt_type in gpt_types: \n",
    "    print(\"\\n\",gpt_type)\n",
    "    for method in [\"sbert\",\"meanLast\"]:    \n",
    "        dataA[\"pred_order\"] = dataA.apply(lambda x: \n",
    "                                      rank_images(\n",
    "                                          sim_scores(x, get_comparator_prefix(x[\"sentence_type\"],gpt_type), method)) \n",
    "                                          , axis=1)\n",
    "        if subset == \"train\":\n",
    "            mydata = only_train(dataA)\n",
    "        else:\n",
    "            mydata = only_subset(dataA,subset)\n",
    "\n",
    "\n",
    "        print(\"Method: \", method, \"on\", subset)\n",
    "        print(\"top1\", top1accuracy(mydata[\"expected_order\"], mydata[\"pred_order\"]), \" spearman\", spearman_correlation(mydata[\"expected_order\"], mydata[\"pred_order\"]))\n",
    "\n",
    "\n",
    "# compare to compound embedding\n",
    "print(\"\\nRank images by similarity to compound embedding:\")\n",
    "for gpt_type in [\"meaning\",\"sentence\"]: \n",
    "    print(\"\\n\",gpt_type)\n",
    "    for method in [\"meanLast4\"]:\n",
    "        dataA[\"pred_order\"] = dataA.apply(lambda x:  \n",
    "                                      rank_images(\n",
    "                                          sim_scores_compound(x, get_comparator_prefix(x[\"sentence_type\"],gpt_type), method)), axis=1)\n",
    "    \n",
    "        if subset == \"train\":\n",
    "            mydata = only_train(dataA)\n",
    "        else:\n",
    "            mydata = only_subset(dataA,subset)\n",
    "        print(\"Method: \", method, \"on\", subset)\n",
    "        print(\"top1\", top1accuracy(mydata[\"expected_order\"], mydata[\"pred_order\"]), \" spearman\", spearman_correlation(mydata[\"expected_order\"], mydata[\"pred_order\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3 (Binary classification idiomatic/literal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### sentence embedding versus gpt sentence/meaning embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed = False\n",
    "dataA = load_pkl(preprocessed, checkpoint_ptlm, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence compared to gpt sentence embeddings (classification accuracy)\n",
      "0.829 sentence_embedding_meanSecondToLast\n",
      "0.814 sentence_embedding_meanLast4\n",
      "0.857 sentence_embedding_meanLast\n",
      "0.829 sentence_embedding_lastCLS\n",
      "0.786 sentence_embedding_sbert\n",
      "\n",
      "0.729 meaning_embedding_meanSecondToLast\n",
      "0.714 meaning_embedding_meanLast4\n",
      "0.757 meaning_embedding_meanLast\n",
      "0.657 meaning_embedding_lastCLS\n",
      "0.814 meaning_embedding_sbert\n",
      "\n",
      "0.743 meaning_cutted_embedding_meanSecondToLast\n",
      "0.7 meaning_cutted_embedding_meanLast4\n",
      "0.757 meaning_cutted_embedding_meanLast\n",
      "0.586 meaning_cutted_embedding_lastCLS\n",
      "0.571 meaning_cutted_embedding_sbert\n",
      "\n",
      "0.757 image_embedding_meanSecondToLast\n",
      "0.771 image_embedding_meanLast4\n",
      "0.786 image_embedding_meanLast\n",
      "0.614 image_embedding_lastCLS\n",
      "0.6 image_embedding_sbert\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "# only training data has sentence_type information\n",
    "dataA_train = only_train(dataA)\n",
    "\n",
    "\n",
    "compound_methods = ['meanSecondToLast',#\n",
    "                    'meanLast4',\n",
    "                    'meanLast',\n",
    "                    #'meanFirst'\n",
    "                    ]\n",
    "methods = ['meanSecondToLast',\n",
    "           'meanLast4',\n",
    "           'meanLast',#'meanFirst',\n",
    "           'lastCLS','sbert']\n",
    "\n",
    "# systematic analysis sentence compared to gpt sentence embeddings\n",
    "gpt_sents = [\"sentence\", \"meaning\", \"meaning_cutted\", \"image\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"sentence compared to gpt sentence embeddings (classification accuracy)\")\n",
    "\n",
    "for sent in gpt_sents:\n",
    "    for m in methods:\n",
    "        pred = dataA.apply(lambda x: compare(x[\"sentence_embedding_\" + m],\n",
    "                                         x[\"gpt_literal_\" + sent + \"_embedding_\" + m], \n",
    "                                         x[\"gpt_idiomatic_\" + sent + \"_embedding_\" + m]), axis=1)\n",
    "        pred = binary2values(pred,\"literal\",\"idiomatic\")\n",
    "        dataA[\"pred_sentence_embedding_compared_to_gpt_\" + sent + \"_embedding_\" +m  ] =pred\n",
    "\n",
    "        dataA_train = only_train(dataA)\n",
    "        pred = dataA_train[\"pred_sentence_embedding_compared_to_gpt_\" + sent + \"_embedding_\" +m ]\n",
    "        # evaluate\n",
    "        y = dataA_train[\"sentence_type\"]\n",
    "        print(round(accuracy_score(y, pred),3),sent + \"_embedding_\" + m)\n",
    "    print(\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence_embedding_meanLast\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   idiomatic       0.87      0.87      0.87        39\n",
      "     literal       0.84      0.84      0.84        31\n",
      "\n",
      "    accuracy                           0.86        70\n",
      "   macro avg       0.86      0.86      0.86        70\n",
      "weighted avg       0.86      0.86      0.86        70\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# classification report for winning system\n",
    "for sent in [\"sentence\"]:\n",
    "    for m in [\"meanLast\"]:\n",
    "        pred = dataA.apply(lambda x: compare(x[\"sentence_embedding_\" + m],\n",
    "                                         x[\"gpt_literal_\" + sent + \"_embedding_\" + m], \n",
    "                                         x[\"gpt_idiomatic_\" + sent + \"_embedding_\" + m]), axis=1)\n",
    "        pred = binary2values(pred,\"literal\",\"idiomatic\")\n",
    "        dataA[\"pred_sentence_embedding_compared_to_gpt_\" + sent + \"_embedding_\" +m  ] =pred\n",
    "\n",
    "        dataA_train = only_train(dataA)\n",
    "        pred = dataA_train[\"pred_sentence_embedding_compared_to_gpt_\" + sent + \"_embedding_\" +m ]\n",
    "        # evaluate\n",
    "        print(sent + \"_embedding_\" + m)\n",
    "        y = dataA_train[\"sentence_type\"]\n",
    "        print(classification_report(y, pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### compound in sentence embedding versus compound in gpt sentence/meaning embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9 sentence_embedding_meanSecondToLast\n",
      "0.9 sentence_embedding_meanLast4\n",
      "0.857 sentence_embedding_meanLast\n",
      "0.743 meaning_embedding_meanSecondToLast\n",
      "0.743 meaning_embedding_meanLast4\n",
      "0.671 meaning_embedding_meanLast\n"
     ]
    }
   ],
   "source": [
    "# compare compound embeddings\n",
    "gpt_sents = [\"sentence\", \"meaning\"]\n",
    "\n",
    "\n",
    "\n",
    "for sent in gpt_sents:\n",
    "    for m in compound_methods:\n",
    "        pred = dataA.apply(lambda x: compare(x[\"compound_embedding_sentence_\" + m],\n",
    "                                         x[\"compound_embedding_gpt_literal_\" + sent + \"_\" + m], \n",
    "                                         x[\"compound_embedding_gpt_idiomatic_\" + sent + \"_\" + m]), axis=1)\n",
    "        pred = binary2values(pred,\"literal\",\"idiomatic\")\n",
    "        dataA[\"pred_compound_embedding_sentence_compared_to_compound_embedding_gpt_\" + sent + \"_\" +m  ] = pred\n",
    "\n",
    "        dataA_train = only_train(dataA)\n",
    "        pred = dataA_train[\"pred_compound_embedding_sentence_compared_to_compound_embedding_gpt_\" + sent + \"_\" +m  ] \n",
    "\n",
    "        # evaluate\n",
    "        y = dataA_train[\"sentence_type\"]\n",
    "        print(round(accuracy_score(y, pred),3),sent + \"_embedding_\" + m)\n",
    "\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compound in sentence compared to gpt compound in sentence embeddings\n",
      "sentence_embedding_meanLast4\n",
      "0.9\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   idiomatic       0.92      0.90      0.91        39\n",
      "     literal       0.88      0.90      0.89        31\n",
      "\n",
      "    accuracy                           0.90        70\n",
      "   macro avg       0.90      0.90      0.90        70\n",
      "weighted avg       0.90      0.90      0.90        70\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# systematic analysis compound embeddings of winning systems\n",
    "gpt_sents = [\"sentence\"]\n",
    "\n",
    "print(\"compound in sentence compared to gpt compound in sentence embeddings\")\n",
    "for sent in gpt_sents:\n",
    "    for m in [\"meanLast4\"]:\n",
    "\n",
    "        dataA_train = only_train(dataA)\n",
    "        pred = dataA_train[\"pred_compound_embedding_sentence_compared_to_compound_embedding_gpt_\" + sent + \"_\" +m  ] \n",
    "\n",
    "        # evaluate\n",
    "        print(sent + \"_embedding_\" + m)\n",
    "        y = dataA_train[\"sentence_type\"]\n",
    "        print(round(accuracy_score(y, pred),3))\n",
    "        print(classification_report(y, pred,zero_division=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 4 (ranking strategies) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline ranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dependent_preds(current, comparator, method):\n",
    "    pred = current[\"binary_pred\"]\n",
    "    return rank_images(sim_scores(current, \"gpt_\" + pred + \"_\" + comparator , method))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select best classificator:\n",
    "\n",
    "pred_all = dataA[\"pred_compound_embedding_sentence_compared_to_compound_embedding_gpt_sentence_meanLast4\"] \n",
    "dataA[\"binary_pred\"] = pred_all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rank images by similarity to original sentence (baseline ranker, no GPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline ranker (rank images by similarity to original sentence):\n",
      "train\n",
      "top1 accuracy 0.4\n",
      "spearman correlation 0.2014\n",
      "Test\n",
      "top1 accuracy 0.2667\n",
      "spearman correlation 0.0533\n",
      "Extended Evaluation\n",
      "top1 accuracy 0.41\n",
      "spearman correlation 0.169\n"
     ]
    }
   ],
   "source": [
    "method = \"sbert\" # \"lastCLS\"\n",
    "dataA[\"pred_order\"] = dataA.apply(lambda x: rank_images(sim_scores(x, \"sentence\", method)), axis=1)\n",
    "\n",
    "dataA_train = only_train(dataA)\n",
    "print(\"baseline ranker (rank images by similarity to original sentence):\")\n",
    "print(\"train\")\n",
    "print(\"top1 accuracy\", top1accuracy(dataA_train[\"expected_order\"], dataA_train[\"pred_order\"]))\n",
    "print(\"spearman correlation\", spearman_correlation(dataA_train[\"expected_order\"], dataA_train[\"pred_order\"]))\n",
    "\n",
    "\n",
    "for s in ['Test', 'Extended Evaluation']:\n",
    "    print(s)\n",
    "    mydata = only_subset(dataA,s)\n",
    "    print(\"top1 accuracy\", top1accuracy(mydata[\"expected_order\"], mydata[\"pred_order\"]))\n",
    "    print(\"spearman correlation\", spearman_correlation(mydata[\"expected_order\"], mydata[\"pred_order\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### baseline ranker + use GPT-data and idiomaticity classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Depending on binary prediction (literal/idiomatic) rank images by similarity to gpt_sentence\n",
      "top1 accuracy 0.3429\n",
      "spearman correlation 0.0557\n",
      "\n",
      "Depending on binary prediction (literal/idiomatic) rank images by similarity to gpt_meaning\n",
      "top1 accuracy 0.3714\n",
      "spearman correlation 0.1829\n",
      "\n",
      "Depending on binary prediction (literal/idiomatic) rank images by similarity to gpt_image\n",
      "top1 accuracy 0.5571\n",
      "spearman correlation 0.0857\n"
     ]
    }
   ],
   "source": [
    "method = \"sbert\" #\"meanLast4\" # \"meanLast\" #\n",
    "for sent_type in ['sentence', 'meaning','image']:\n",
    "    print(\"\\nDepending on binary prediction (literal/idiomatic) rank images by similarity to gpt_\" + sent_type)\n",
    "    dataA[\"pred_order_dependent\"] = dataA.apply(lambda x: dependent_preds(x,sent_type, method), axis=1)\n",
    "\n",
    "    dataA_train = only_train(dataA)\n",
    "    print(\"top1 accuracy\", top1accuracy(dataA_train[\"expected_order\"], dataA_train[\"pred_order_dependent\"]))\n",
    "    print(\"spearman correlation\", spearman_correlation(dataA_train[\"expected_order\"], dataA_train[\"pred_order_dependent\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline ranker to gpt-caption\n",
      "train\n",
      "top1 accuracy 0.5571\n",
      "spearman correlation 0.0857\n",
      "Test\n",
      "top1 accuracy 0.4667\n",
      "spearman correlation 0.0267\n",
      "Extended Evaluation\n",
      "top1 accuracy 0.48\n",
      "spearman correlation 0.14\n"
     ]
    }
   ],
   "source": [
    "method = \"sbert\" \n",
    "sent_type  = 'image'\n",
    "dataA[\"pred_order_dependent\"] = dataA.apply(lambda x: dependent_preds(x,sent_type, method), axis=1)\n",
    "\n",
    "dataA_train = only_train(dataA)\n",
    "print(\"baseline ranker to gpt-caption\")\n",
    "print(\"train\")\n",
    "print(\"top1 accuracy\", top1accuracy(dataA_train[\"expected_order\"], dataA_train[\"pred_order_dependent\"]))\n",
    "print(\"spearman correlation\", spearman_correlation(dataA_train[\"expected_order\"], dataA_train[\"pred_order_dependent\"]))\n",
    "\n",
    "for s in ['Test', 'Extended Evaluation']:\n",
    "    print(s)\n",
    "    mydata = only_subset(dataA,s)\n",
    "    print(\"top1 accuracy\", top1accuracy(mydata[\"expected_order\"], mydata[\"pred_order_dependent\"]))\n",
    "    print(\"spearman correlation\", spearman_correlation(mydata[\"expected_order\"], mydata[\"pred_order_dependent\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking images by similarity to gpt material -- pair ranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pair of most literal images, pair of most idiomatic image, unrelated\n",
    "\n",
    "def dependent_preds_compare_pairs(current, sent_type,method):\n",
    "    preds = [0 for i in range(5)]\n",
    "    # similarities of all captions with gpt_literal_\"method\" as a dictionary with keys = image names and values = sim. scores\n",
    "    scores_lit = sim_scores(current, \"gpt_literal_\" + sent_type,method)  \n",
    "    # similarities of all captions to gpt_idiomatic_\"method\"\n",
    "    scores_id = sim_scores(current, \"gpt_idiomatic_\" + sent_type,method)\n",
    "    image_names = list(scores_lit.keys())\n",
    "    scores = {\"literal\": scores_lit, \"idiomatic\": scores_id}\n",
    "    type = current[\"binary_pred\"] # idiomatic/literal classification\n",
    "    if type ==\"idiomatic\":\n",
    "        nottype = \"literal\"\n",
    "    else:\n",
    "        nottype = \"idiomatic\"\n",
    "    \n",
    "    max_type = max(scores[type],key=scores[type].get)\n",
    "    preds[0] = max_type\n",
    "    del scores[nottype][max_type]\n",
    "    max_nottype = max(scores[nottype],key=scores[nottype].get)\n",
    "    preds[3] = max_nottype\n",
    "    \n",
    "    # score images by similarity to most literal and most idiomatic image\n",
    "    max_type_index = image_names.index(max_type)\n",
    "    max_nottype_index = image_names.index(max_nottype)\n",
    "    scores_images_type = sim_scores(current, \"image\" + str(max_type_index + 1) + \"_caption\",method)\n",
    "    scores_images_nottype = sim_scores(current, \"image\" + str(max_nottype_index +1) + \"_caption\",method)\n",
    "    del scores_images_type[max_type]\n",
    "    del scores_images_type[max_nottype]\n",
    "    del scores_images_nottype[max_type]\n",
    "    del scores_images_nottype[max_nottype]\n",
    "\n",
    "    # choose most similar to max_type and to max_nottype\n",
    "    sim_max_type = max(scores_images_type, key=scores_images_type.get)\n",
    "    preds[1] = sim_max_type\n",
    "\n",
    "    del scores_images_nottype[sim_max_type]\n",
    "    sim_max_nottype = max(scores_images_nottype, key=scores_images_nottype.get)\n",
    "    preds[2] = sim_max_nottype\n",
    "    # remaining image is unrelated image\n",
    "    preds[4] = list(set(image_names).difference(set([max_type,max_nottype,sim_max_type,sim_max_nottype])))[0]\n",
    "\n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pair ranker: Evaluation on training data (rank images dependent on binary classification. For order use inter-image similarity):\n",
      "top1 accuracy 0.5571\n",
      "spearman correlation 0.3243\n",
      "Test\n",
      "top1 accuracy 0.4667\n",
      "spearman correlation 0.18\n",
      "Extended Evaluation\n",
      "top1 accuracy 0.48\n",
      "spearman correlation 0.298\n"
     ]
    }
   ],
   "source": [
    "method = \"sbert\" # \"meanLast\" #  \n",
    "sent_type = \"image\" #  \"meaning\" # \"sentence\" #  \n",
    "dataA[\"pred_order_dependent\"] = dataA.apply(lambda x: dependent_preds_compare_pairs(x,sent_type,method), axis=1)\n",
    "\n",
    "dataA_train = only_train(dataA)\n",
    "print(\"pair ranker: Evaluation on training data (rank images dependent on binary classification. For order use inter-image similarity):\")\n",
    "print(\"top1 accuracy\", top1accuracy(dataA_train[\"expected_order\"], dataA_train[\"pred_order_dependent\"]))\n",
    "print(\"spearman correlation\", spearman_correlation(dataA_train[\"expected_order\"], dataA_train[\"pred_order_dependent\"]))\n",
    "\n",
    "for s in ['Test', 'Extended Evaluation']:\n",
    "    print(s)\n",
    "    mydata = only_subset(dataA,s)\n",
    "    print(\"top1 accuracy\", top1accuracy(mydata[\"expected_order\"], mydata[\"pred_order_dependent\"]))\n",
    "    print(\"spearman correlation\", spearman_correlation(mydata[\"expected_order\"], mydata[\"pred_order_dependent\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking images by similarity to gpt material -- extreme ranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sim scores\n",
    "\n",
    "def dependent_preds_no_pairs(current, sent_type, method):\n",
    "    preds = [0 for i in range(5)]\n",
    "    # similarities of all captions with gpt_literal_\"method\" as a dictionary with keys = image names and values = sim. scores\n",
    "    scores_lit = sim_scores(current, \"gpt_literal_\" + sent_type,method)  \n",
    "    # similarities of all captions to gpt_idiomatic_\"method\"\n",
    "    scores_id = sim_scores(current, \"gpt_idiomatic_\" + sent_type,method)\n",
    "    image_names = list(scores_lit.keys())\n",
    "    scores = {\"literal\": scores_lit, \"idiomatic\": scores_id}\n",
    "    type = current[\"binary_pred\"] # idiomatic/literal classification\n",
    "    if type ==\"idiomatic\":\n",
    "        nottype = \"literal\"\n",
    "    else:\n",
    "        nottype = \"idiomatic\"\n",
    "\n",
    "    # get highest scoring image for literal and idiomatic\n",
    "    max_type = max(scores[type],key=scores[type].get)\n",
    "    preds[0] = max_type\n",
    "    del scores[nottype][max_type]\n",
    "    max_nottype = max(scores[nottype],key=scores[nottype].get)\n",
    "    preds[3] = max_nottype\n",
    "    del scores[type][max_type]\n",
    "    del scores[type][max_nottype]\n",
    "\n",
    "    # get second highest scoring lit and id\n",
    "    max_type_second = max(scores[type], key=scores[type].get)\n",
    "    preds[1] = max_type_second\n",
    "    del scores[nottype][max_nottype]\n",
    "    del scores[nottype][max_type_second]\n",
    "    max_nottype_second = max(scores[nottype], key=scores[nottype].get)\n",
    "    preds[2] = max_nottype_second\n",
    "    \n",
    "    preds[4] = list(set(image_names).difference(set([max_type, max_nottype, max_type_second, max_nottype_second])))[0]\n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extreme ranker: Evaluation on training data (rank images dependent on binary classification. For order use similarity to gpt_image only):\n",
      "top1 accuracy 0.5571\n",
      "spearman correlation 0.2457\n",
      "Test\n",
      "top1 accuracy 0.4667\n",
      "spearman correlation 0.0867\n",
      "Extended Evaluation\n",
      "top1 accuracy 0.48\n",
      "spearman correlation 0.334\n"
     ]
    }
   ],
   "source": [
    "method = \"sbert\" # \"meanLast\" # \n",
    "dataA[\"pred_order_dependent\"] = dataA.apply(lambda x: dependent_preds_no_pairs(x,\"image\", method), axis=1)\n",
    "\n",
    "dataA_train = only_train(dataA)\n",
    "print(\"extreme ranker: Evaluation on training data (rank images dependent on binary classification. For order use similarity to gpt_image only):\")\n",
    "print(\"top1 accuracy\", top1accuracy(dataA_train[\"expected_order\"], dataA_train[\"pred_order_dependent\"]))\n",
    "print(\"spearman correlation\", spearman_correlation(dataA_train[\"expected_order\"], dataA_train[\"pred_order_dependent\"]))\n",
    "#make_submission(dataA,\"pred_order_dependent\", \"Dev\")\n",
    "\n",
    "for s in ['Test', 'Extended Evaluation']:\n",
    "    print(s)\n",
    "    mydata = only_subset(dataA,s)\n",
    "    print(\"top1 accuracy\", top1accuracy(mydata[\"expected_order\"], mydata[\"pred_order_dependent\"]))\n",
    "    print(\"spearman correlation\", spearman_correlation(mydata[\"expected_order\"], mydata[\"pred_order_dependent\"]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
