{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subtask B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter setting\n",
    "\n",
    "# model\n",
    "checkpoint = 'bert-base-uncased' \n",
    "#checkpoint ='jlsalim/bert-uncased-idiomatic-literal-recognizer'\n",
    "checkpoint_write = checkpoint.replace(\"/\", \"_\").replace(\"\\\\\", \"_\")\n",
    "\n",
    "\n",
    "# full sentences or preprocessed (filtered to content words of specific POS)\n",
    "preprocessed = False\n",
    "#preprocessed = True\n",
    "\n",
    "#remove_CLS_SEP = True \n",
    "remove_CLS_SEP = False\n",
    "\n",
    "# read tsv file\n",
    "import csv\n",
    "import sys\n",
    "import ast\n",
    "\n",
    "import pandas as pd\n",
    "dataDirectory = \"./data/\"\n",
    "\n",
    "# read in competition data\n",
    "dataB_train = pd.read_csv(dataDirectory + \"subtask_b_train.tsv\", sep='\\t')\n",
    "dataB_dev = pd.read_csv(dataDirectory + \"subtask_b_dev.tsv\", sep='\\t')\n",
    "dataB_test = pd.read_csv(dataDirectory +\"subtask_b_test.tsv\", sep='\\t')\n",
    "dataB_xe = pd.read_csv(dataDirectory + \"subtask_b_xe.tsv\", sep='\\t')\n",
    "\n",
    "dataB = pd.concat([dataB_train,dataB_dev,dataB_test,dataB_xe])\n",
    "# reset index\n",
    "dataB = dataB.reset_index(drop=True)\n",
    "\n",
    "# read in chatGPT data from csv\n",
    "data_chatGPT_image_b = pd.read_csv(dataDirectory + \"gpt_image_descriptions_all_taskB.csv\")\n",
    "data_chatGPT_b = pd.read_csv(dataDirectory + \"chatGPTNew_taskB_all.csv\")\n",
    "\n",
    "# rename each column with \"gpt_\" in front of the column name\n",
    "data_chatGPT_b.rename(columns=lambda x: 'gpt_' + x, inplace=True)\n",
    "data_chatGPT_image_b.rename(columns={\"idiomatic_image\": \"gpt_idiomatic_image\",\"literal_image\": \"gpt_literal_image\"}, inplace=True)\n",
    "\n",
    "\n",
    "# inserting the missing compound column \n",
    "data_chatGPT_b[\"compound\"] = [None for i in range(len(data_chatGPT_b))]\n",
    "for i in range(len(data_chatGPT_b)):\n",
    "    data_chatGPT_b[\"compound\"][i] = data_chatGPT_b[\"gpt_idiomatic_meaning\"][i].split(\" is\")[0].strip().lower()\n",
    "\n",
    "\n",
    "# cleanup data\n",
    "# replace ’ with ' in all columns\n",
    "\n",
    "# merge data into one dataframe\n",
    "dataB = pd.merge(dataB, data_chatGPT_b, on='compound')\n",
    "dataB = pd.merge(dataB, data_chatGPT_image_b, on='compound')\n",
    "\n",
    "for column in dataB.columns:\n",
    "    dataB[column] = dataB[column].str.replace(\"’\",\"'\")\n",
    "\n",
    "sentence_type_columns = ['sequence_caption1', 'sequence_caption2',        \n",
    "                         'image1_caption', 'image2_caption', 'image3_caption', 'image4_caption',\n",
    "                        'gpt_idiomatic_meaning', 'gpt_literal_meaning',\n",
    "                        'gpt_idiomatic_sentence', 'gpt_literal_sentence', \n",
    "                        'gpt_idiomatic_image','gpt_literal_image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the dataframe sample and train (data items that have literal/idiomatic information given)\n",
    "def only_train(dataA): # returns the dataframe sample and train (data items that have literal/idiomatic information given)\n",
    "    dataA = pd.concat([dataA[dataA[\"subset\"] == \"Sample\"],dataA[dataA[\"subset\"]== \"Train\"]])\n",
    "    dataA = dataA.reset_index(drop=True)\n",
    "    return dataA\n",
    "\n",
    "# returns the dataframe of subset\n",
    "def only_subset(dataA, subset): # returns the dataframe sample and train (data items that have literal/idiomatic information given)\n",
    "    return dataA[dataA[\"subset\"] == subset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['compound', 'subset', 'sentence_type', 'expected_item',\n",
       "       'sequence_caption1', 'sequence_caption2', 'image1_name',\n",
       "       'image1_caption', 'image2_name', 'image2_caption', 'image3_name',\n",
       "       'image3_caption', 'image4_name', 'image4_caption',\n",
       "       'gpt_idiomatic_meaning', 'gpt_literal_meaning',\n",
       "       'gpt_idiomatic_sentence', 'gpt_literal_sentence', 'gpt_idiomatic_image',\n",
       "       'gpt_literal_image'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataB.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Extended Evaluation    30\n",
       "Sample                 10\n",
       "Train                  10\n",
       "Dev                     5\n",
       "Test                    5\n",
       "Name: subset, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataB[\"subset\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample\n",
      "idiomatic 7\n",
      "literal 3\n",
      "============================================================\n",
      "Train\n",
      "idiomatic 6\n",
      "literal 4\n",
      "============================================================\n",
      "Dev\n",
      "idiomatic 2\n",
      "literal 3\n",
      "============================================================\n",
      "Test\n",
      "idiomatic 0\n",
      "literal 0\n",
      "============================================================\n",
      "Extended Evaluation\n",
      "idiomatic 12\n",
      "literal 18\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "for subset in [\"Sample\",\"Train\",\"Dev\",\"Test\",\"Extended Evaluation\"]:\n",
    "    print(subset)\n",
    "    mydata = dataB[dataB[\"subset\"] == subset]\n",
    "    idiomatic = mydata[mydata[\"sentence_type\"] == \"idiomatic\"]\n",
    "    literal = mydata[mydata[\"sentence_type\"] == \"literal\"]\n",
    "    print(\"idiomatic\", idiomatic.shape[0])\n",
    "    print(\"literal\", literal.shape[0])\n",
    "    print(\"===\"*20)\n",
    "\n",
    "# for test data manually annotated: 3 idiomatic, 2 literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing of text\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download necessary NLTK data\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "def prepare_text(raw_text):\n",
    "    \n",
    "    # Convert text to lowercase and remove punctuation\n",
    "    normalized_text = raw_text.lower()\n",
    "    normalized_text = re.sub(r\"[^a-zA-Z\\s]\", \"\", normalized_text)\n",
    "\n",
    "    # Tokenize the normalized text\n",
    "    tokens = word_tokenize(normalized_text)\n",
    "\n",
    "    # Apply POS tagging and retain only nouns, verbs\n",
    "    pos_tags = nltk.pos_tag(tokens, tagset='universal')\n",
    "    pos_tags_to_keep = {\"NOUN\", \"VERB\", \"ADJ\"}\n",
    "#    pos_tags_to_keep = {\"NOUN\", \"VERB\", \"ADV\", \"ADJ\"}\n",
    "    filtered_tokens = [word for word, pos in pos_tags if pos in pos_tags_to_keep]\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_text = [word for word in filtered_tokens if word.lower() not in stop_words]\n",
    "\n",
    "    # Lemmatize the remaining tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_text = [lemmatizer.lemmatize(word) for word in filtered_text]\n",
    "    \n",
    "    return \" \".join(lemmatized_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea: we use a pretrained BERT model to generate embeddings of sentences and of the compound in the context of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from transformers import AutoModel\n",
    "\n",
    "# model is selected from https://huggingface.co/models\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModel.from_pretrained(checkpoint, output_hidden_states=True).to(device)\n",
    "model = model.eval()\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch, padding=True, truncation=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Computing BERT sentence-based embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_attention_tokens(attention_mask_item):\n",
    "    attention_mask_new = attention_mask_item.clone()\n",
    "    one_indices = (attention_mask_item == 1).nonzero(as_tuple=True)[0]\n",
    "    if remove_CLS_SEP == True:\n",
    "        # Setze die erste und letzte 1 auf 0: [CLS] und [SEP] Tokens\n",
    "        attention_mask_new[one_indices[0]] = 0  # Erste 1\n",
    "        attention_mask_new[one_indices[-1]] = 0  # Letzte 1\n",
    "    return attention_mask_new\n",
    "\n",
    "\n",
    "# different pooling methods for embeddings are computed\n",
    "# NOTE: padding tokens should be excluded (not done yet)\n",
    "\n",
    "def get_sentence_embedding(hidden_states,method,attention_mask):\n",
    "    sentence_embedding = []\n",
    "    if method == 'meanLast4': # average of all tokens of the last 4 layers\n",
    "        for i in range(len(hidden_states[0])):\n",
    "            # token_vecs is mean of last 4 layers\n",
    "            token_tensor = torch.stack([hidden_states[-1][i], hidden_states[-2][i], hidden_states[-3][i], hidden_states[-4][i]], dim=0)\n",
    "            token_vecs = torch.mean(token_tensor, dim=0)\n",
    "            attention = get_attention_tokens(attention_mask[i]) \n",
    "            token_vecs = token_vecs[attention.bool()]\n",
    "            sentence_embedding.append(torch.mean(token_vecs, dim=0))\n",
    "    elif method == 'meanSecondToLast': # average of second to last layer\n",
    "        for i in range(len(hidden_states[-2])):\n",
    "            token_vecs = hidden_states[-2][i]\n",
    "            attention = get_attention_tokens(attention_mask[i]) \n",
    "            token_vecs = token_vecs[attention.bool()]\n",
    "            sentence_embedding.append(torch.mean(token_vecs, dim=0))\n",
    "    elif method == 'meanLast': # average of last layer\n",
    "        for i in range(len(hidden_states[-1])):\n",
    "            token_vecs = hidden_states[-1][i]\n",
    "            attention = get_attention_tokens(attention_mask[i]) \n",
    "            token_vecs = token_vecs[attention.bool()]\n",
    "            sentence_embedding.append(torch.mean(token_vecs, dim=0))\n",
    "    elif method == 'lastCLS': # CLS token of last layer\n",
    "        sentence_embedding = hidden_states[-1][:, 0, :]\n",
    "    elif method == 'meanFirst': # average of first layer\n",
    "        for i in range(len(hidden_states[0])):\n",
    "            token_vecs = hidden_states[0][i]\n",
    "            attention = get_attention_tokens(attention_mask[i]) \n",
    "            token_vecs = token_vecs[attention.bool()]\n",
    "            sentence_embedding.append(torch.mean(token_vecs, dim=0))\n",
    "    elif method == 'firstCLS': # CLS token of first layer\n",
    "        sentence_embedding = hidden_states[0][:, 0, :]\n",
    "    return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_caption1\n",
      "sequence_caption2\n",
      "image1_caption\n",
      "image2_caption\n",
      "image3_caption\n",
      "image4_caption\n",
      "gpt_idiomatic_meaning\n",
      "gpt_literal_meaning\n",
      "gpt_idiomatic_sentence\n",
      "gpt_literal_sentence\n",
      "gpt_idiomatic_image\n",
      "gpt_literal_image\n"
     ]
    }
   ],
   "source": [
    "methods = ['meanSecondToLast','meanLast4','meanLast','lastCLS']\n",
    "\n",
    "for column in sentence_type_columns:\n",
    "    print(column)\n",
    "    dataB_sentence_tokenized = tokenize(dataB[column].tolist())\n",
    "    \n",
    "    # convert input_ids to tensor\n",
    "    input_ids_sentence = torch.tensor(dataB_sentence_tokenized[\"input_ids\"]).to(device)\n",
    "    attention_mask_sentence = torch.tensor(dataB_sentence_tokenized[\"attention_mask\"]).to(device)\n",
    "\n",
    "    # pass input_ids to model\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids_sentence, attention_mask_sentence)\n",
    "    \n",
    "    hidden_states_sentence = output.hidden_states\n",
    "\n",
    "    # use all methods for getting sentence embeddings and add them to dataB\n",
    "\n",
    "\n",
    "    for method in methods:\n",
    "        X = get_sentence_embedding(hidden_states_sentence,method,attention_mask_sentence)\n",
    "        X = np.array([x.cpu().numpy() for x in X]).tolist()\n",
    "        # add a new column to dataB \n",
    "        dataB[column + '_embedding_'+ method] = X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing BERT compound-based embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes the compound occurs in the sentence only in plural form. So both forms are needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##!pip install inflect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sometimes the compound occurs in plural form in the sentence\n",
    "\n",
    "# add a new column to dataB with the plural form of the compound \n",
    "\n",
    "\n",
    "from re import *\n",
    "import inflect\n",
    "\n",
    "engine = inflect.engine()\n",
    "\n",
    "dataB[\"compound_plural\"] = [None for i in range(len(dataB))]\n",
    "\n",
    "for i in range(len(dataB[\"compound\"])):\n",
    "    dataB[\"compound_plural\"][i] = engine.plural(dataB[\"compound\"][i])\n",
    "\n",
    "# tokenize all compounds (original and plural)\n",
    "dataB_compound_tokenized = tokenize(dataB[\"compound\"].tolist())\n",
    "dataB_compound_plural_tokenized = tokenize(dataB[\"compound_plural\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the index of the compound in the sentence\n",
    "def get_idx(compound_tokens, compound_plural_tokens, sentence_tokens):\n",
    "    # remove 0-tokens from compound_tokens (removes tokens that are due to padding)\n",
    "    compound_tokens = [i for i in compound_tokens if i != 0]\n",
    "    # remove [CLS] and [SEP] from compound_tokens\n",
    "    compound_tokens = compound_tokens[1:-1]\n",
    "    compound_plural_tokens = [i for i in compound_plural_tokens if i != 0]\n",
    "    compound_plural_tokens = compound_plural_tokens[1:-1]\n",
    "    idx = []\n",
    "    # find the first occurence of the sequence of compound_tokens in sentence_tokens (singular and plural forms)\n",
    "    for i in range(len(sentence_tokens)):\n",
    "        if sentence_tokens[i:i+len(compound_tokens)] == compound_tokens:\n",
    "            for j in range(i, i+ len(compound_tokens)):\n",
    "                idx.append(j)\n",
    "    for i in range(len(sentence_tokens)):\n",
    "        if sentence_tokens[i:i+len(compound_plural_tokens)] == compound_plural_tokens:\n",
    "            for j in range(i, i+ len(compound_plural_tokens)):\n",
    "                idx.append(j)\n",
    "    # remove duplicates from idx\n",
    "    idx = list(set(idx))\n",
    "    return idx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the embeddings of the tokens in idxList. \n",
    "# The embeddings are combined to a single embedding by different averaging methods\n",
    "import numpy as np\n",
    "def get_idxList_embedding(hidden_states,idxLists,method):\n",
    "    embedding = []\n",
    "    if method == 'meanLast4':\n",
    "        for i in range(len(hidden_states[-1])):\n",
    "            # token_vecs is mean of last 4 layers\n",
    "            idxList = idxLists[i]\n",
    "            token_tensor = torch.stack([hidden_states[-1][i][idxList], hidden_states[-2][i][idxList], hidden_states[-3][i][idxList], hidden_states[-4][i][idxList]], dim=0)\n",
    "            token_vecs = torch.mean(token_tensor, dim=0)\n",
    "            embedding.append(torch.mean(token_vecs, dim=0).tolist())\n",
    "    elif method == 'meanSecondToLast':\n",
    "        for i in range(len(hidden_states[-2])):\n",
    "            idxList = idxLists[i]\n",
    "            token_vecs = hidden_states[-2][i][idxList]\n",
    "            embedding.append(torch.mean(token_vecs, dim=0).tolist())\n",
    "    elif method == 'meanLast':\n",
    "        for i in range(len(hidden_states[-1])):\n",
    "            idxList = idxLists[i]\n",
    "            token_vecs = hidden_states[-1][i][idxList]\n",
    "            embedding.append(torch.mean(token_vecs, dim=0).tolist())\n",
    "    elif method == 'meanFirst':\n",
    "        for i in range(len(hidden_states[0])):\n",
    "            idxList = idxLists[i]\n",
    "            token_vecs = hidden_states[0][i][idxList]\n",
    "            embedding.append(torch.mean(token_vecs, dim=0).tolist())\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_type_columns = [#'sequence_caption1', 'sequence_caption2',        \n",
    "#                         'image1_caption', 'image2_caption', 'image3_caption', 'image4_caption',\n",
    "                        'gpt_idiomatic_meaning', 'gpt_literal_meaning',\n",
    "                        'gpt_idiomatic_sentence', 'gpt_literal_sentence', \n",
    "#                       'gpt_idiomatic_image','gpt_literal_image'\n",
    "                        ]\n",
    "\n",
    "compound_methods = ['meanSecondToLast','meanLast4','meanLast']\n",
    "\n",
    "compound_tokenized = tokenize(dataB[\"compound\"].tolist())\n",
    "compound_plural_tokenized = tokenize(dataB[\"compound_plural\"].tolist())    \n",
    "\n",
    "for column in sentence_type_columns:\n",
    "    # tokenize the column\n",
    "    tokenized = tokenize(dataB[column].tolist())\n",
    "\n",
    "    # hidden states for gpt_Meaning\n",
    "    input_ids = torch.tensor(tokenized[\"input_ids\"]).to(device)\n",
    "    attention_mask = torch.tensor(tokenized[\"attention_mask\"]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids, attention_mask)\n",
    "\n",
    "    hidden_states = output.hidden_states\n",
    " \n",
    "    # add gpt_compound_embeddings\n",
    " \n",
    "    # add column to dataB with the indices of the compound in the sentence\n",
    "    dataB[column + \"_compound_idx\"] = [get_idx(compound_tokenized[\"input_ids\"][i], \n",
    "                                               compound_plural_tokenized[\"input_ids\"][i], \n",
    "                                               tokenized[\"input_ids\"][i]) for i in range(len(dataB))]\n",
    "    \n",
    "    # apply the methods in compound_methods to get the embeddings of the compound\n",
    "    for method in compound_methods:\n",
    "        dataB[ column + \"_embedding_\"+ method + \"_compound\"] = get_idxList_embedding(hidden_states,\n",
    "                                                                                  dataB[column + \"_compound_idx\"],\n",
    "                                                                                  method) \n",
    "    dataB = dataB.copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt_literal_sentence\n",
      "swan song\n",
      "The swan swam gracefully on the lake, its song heard from the shore.\n",
      "26\n",
      "gpt_literal_sentence\n",
      "swan song\n",
      "The swan swam gracefully on the lake, its song heard from the shore.\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "# print  if compound_idx is empty (ideally there should be no empty compound_idx)\n",
    "for column in sentence_type_columns:\n",
    "    for i in range(len(dataB)):\n",
    "        if len(dataB[column + \"_compound_idx\"][i]) == 0:\n",
    "            print(column)\n",
    "            print(dataB[\"compound\"][i])\n",
    "            print(dataB[column][i])\n",
    "            print(i)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace nan-embeddings (due to missing compound) by corresponding sentence embedding:\n",
    "for column in sentence_type_columns:\n",
    "    for method in compound_methods:\n",
    "        for i in range(len(dataB)):\n",
    "            if len(dataB[column + \"_compound_idx\"][i]) == 0:\n",
    "                dataB[column + \"_embedding_\"+ method + \"_compound\"][i] = dataB[column + \"_embedding_\"+ method][i]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SBert embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0429 12:09:02.522000 9416 site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    }
   ],
   "source": [
    "# pip install -U sentence-transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "\n",
    "\n",
    "sentence_type_columns = ['sequence_caption1', 'sequence_caption2',        \n",
    "                         'image1_caption', 'image2_caption', 'image3_caption', 'image4_caption',\n",
    "                        'gpt_idiomatic_meaning', 'gpt_literal_meaning',\n",
    "                        'gpt_idiomatic_sentence', 'gpt_literal_sentence', \n",
    "                        'gpt_idiomatic_image','gpt_literal_image']\n",
    "\n",
    "#  SBert embeddings are generated  for all sentence like columns\n",
    "for type in sentence_type_columns:\n",
    "    dataB[type + \"_embedding_sbert\"] = dataB[type].apply(lambda x: model.encode(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save dataB pickle\n",
    "import pickle\n",
    "if preprocessed == True:\n",
    "    prep = \"_preprocessed_\"\n",
    "else:\n",
    "    prep = \"_\"\n",
    "\n",
    "if remove_CLS_SEP == True:\n",
    "    cls_sep = \"_without_CLS_SEP\"\n",
    "else:\n",
    "    cls_sep = \"\"\n",
    "\n",
    "\n",
    "dataB.to_pickle(\"dataB\"+ prep + \".pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "def make_submission(data, binary_column, image_column, subset):\n",
    "    if subset == \"Extended Evaluation\":\n",
    "        name = \"xe\"\n",
    "    else:\n",
    "        name = \"EN\"\n",
    "    full_name = \"submission_\" + name\n",
    "    subset_data = only_subset(data,subset)\n",
    "    submission_df = pd.DataFrame()\n",
    "    submission_df[\"compound\"] = subset_data[\"compound\"]\n",
    "    submission_df[\"subset\"] = subset_data[\"subset\"]\n",
    "    submission_df[\"sentence_type\"] = subset_data[binary_column]\n",
    "    submission_df[\"expected_item\"] = subset_data[image_column]\n",
    "    submission_df.to_csv(full_name + \".tsv\", sep=\"\\t\", index=False)\n",
    "    ZipFile(full_name + '.zip', 'w').write(full_name + '.tsv')\n",
    "    print(\"File zipped and saved as \"+ full_name + \".zip\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep = \"_\" # not preprocessed\n",
    "#prep = \"_preprocessed_\"\n",
    "\n",
    "dataB = pd.read_pickle(\"dataB\" + prep + \".pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to display images\n",
    "\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "fileDirectory = 'D:\\\\Wiebke Petersen\\\\Downloads\\\\AdMIRe Subtask B Train\\\\train' # adapt to your file path\n",
    "\n",
    "# Open the image file\n",
    "def display_image(compound, fn):\n",
    "    img = Image.open(fileDirectory + \"\\\\\" + compound + \"\\\\\" + fn)\n",
    "    new_size = (150, 150)  # Width, Height\n",
    "    img_resized = img.resize(new_size)  \n",
    "    # Display the image\n",
    "    display(img_resized)\n",
    "\n",
    "# returns list of image names sorted from image1 to image5\n",
    "def get_image_names(n,mydata):\n",
    "    names = []\n",
    "    for i in [1,2,3,4]:\n",
    "         names.append(mydata['image' + str(i) + '_name'][n])\n",
    "    return names\n",
    "\n",
    "\n",
    "def print_item(n, mydata):\n",
    "    # print  'sentence_type', 'sentence'\n",
    "    compound = mydata['compound'].iloc[n]\n",
    "    print(compound)\n",
    "    print(mydata['sentence_type'].iloc[n])\n",
    "    print(mydata['sequence_caption1'].iloc[n])\n",
    "    display_image(compound,'s1.png')\n",
    "    print(mydata['sequence_caption2'].iloc[n])\n",
    "    display_image(compound,'s2.png')\n",
    "    print(\"=====================================================================\")\n",
    "\n",
    "\n",
    "    # for image_names in 'expected_order' print image_captions\n",
    "    names  =  get_image_names(n,mydata)\n",
    "\n",
    "    for image_name in names:\n",
    "        display_image(compound, image_name)\n",
    "        # get index of image_name in names\n",
    "        index = names.index(image_name) + 1\n",
    "        print(mydata['image'+str(index)+'_caption'][n])\n",
    "    print(\"=====================================================================\")\n",
    "    print(\"expected image: \")\n",
    "    display_image(compound, dataB_train[\"expected_item\"][n])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataB_train = only_train(dataB)\n",
    "#print_item(9,dataB_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(u, v):\n",
    "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))\n",
    "\n",
    "\n",
    "def compare(emb,emb0,emb1):\n",
    "    sim0 = cosine_similarity(emb,emb0)\n",
    "    sim1 = cosine_similarity(emb,emb1)\n",
    "    if sim0 > sim1:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def binary2values(list,val0,val1):\n",
    "    final = []\n",
    "    for element in list:\n",
    "        if element == 1:\n",
    "            final.append(val1)\n",
    "        else:\n",
    "            final.append(val0)\n",
    "    return final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### compare sequence captions with image captions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### choose by maximal similarity sum "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [\"image1\", \"image2\", \"image3\", \"image4\"]\n",
    "sequence_captions = ['sequence_caption1', 'sequence_caption2']\n",
    "\n",
    "# compare image captions to sequence captions (every sequence caption to every candidate caption)\n",
    "def compare_image_to_sequence(current,embedding):\n",
    "    sims = []\n",
    "    for image in images:\n",
    "        seq_sim = []\n",
    "        for seq in sequence_captions:\n",
    "            seq_sim.append(cosine_similarity(current[seq+ \"_embedding_\" + embedding], current[image+ \"_caption_embedding_\" + embedding]))\n",
    "        sims.append(seq_sim)\n",
    "    return sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# decide by maximal sum of similarities   \n",
    "modelmethod =  \"sbert\" # \"meanLast4\" # \"lastCLS\" # \"meanLast\" # \"tfidf\" #\n",
    "dataB[\"pred_image_\"+ modelmethod ] = dataB.apply(lambda x:  \n",
    "                                        np.argmax([sum(i) for i in compare_image_to_sequence(x,modelmethod)]) + 1, \n",
    "                                        axis = 1)\n",
    "dataB[\"pred_image_\" + modelmethod] = dataB.apply(lambda x: x[\"image\" + str(x[\"pred_image_\" + modelmethod]) + \"_name\"], axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score (comparison sequence captions and image caption, embedding: sbert, method: max sum) 0.6\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "dataB_train = only_train(dataB)\n",
    "print(\"accuracy score (comparison sequence captions and image caption, embedding: \" + modelmethod + \", method: max sum)\", \n",
    "      str(accuracy_score(dataB_train[\"expected_item\"], dataB_train[\"pred_image_\" + modelmethod])\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### choose by maximum similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decide by maximum of similarity # winner sbert (0.65)\n",
    "modelmethod =   \"sbert\" #    \"meanLast4\" # \"lastCLS\" #\"meanLast\" # \"tfidf\" # \n",
    "\n",
    "dataB[\"pred_image_\" + modelmethod] = dataB.apply(lambda x:  \n",
    "                                        np.argmax([max(i) for i in compare_image_to_sequence(x,modelmethod)]) + 1, \n",
    "                                        axis = 1)\n",
    "dataB[\"pred_image_\" + modelmethod] = dataB.apply(lambda x: x[\"image\" + str(x[\"pred_image_\" + modelmethod]) + \"_name\"], axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "accuracy score (comparison sequence captions and image caption, embedding: sbert, method: max max) 0.65\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "dataB_train = only_train(dataB)\n",
    "print(\"\\naccuracy score (comparison sequence captions and image caption, embedding: \" + modelmethod + \", method: max max)\", \n",
    "      str(accuracy_score(dataB_train[\"expected_item\"], dataB_train[\"pred_image_\" + modelmethod])\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict idiomatic/literal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### predict by comparison of sequence caption with gpt sentence/meaning/image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_captions = ['sequence_caption1', 'sequence_caption2']\n",
    "bin_types = [\"literal\", \"idiomatic\"]\n",
    "sent_types = [\"meaning\", \"sentence\", \"image\"]                        \n",
    "\n",
    "def compare_sequence_to_gpt(current, embedding):\n",
    "    sim = {}\n",
    "    for sent in sent_types:\n",
    "        sim_sent = []\n",
    "        for bin in bin_types:\n",
    "            sim_bin = []\n",
    "            for seq in sequence_captions:\n",
    "                sim_bin.append(\n",
    "                    cosine_similarity(current[\"gpt_\" + bin + \"_\" + sent +  \"_embedding_\" + embedding], \n",
    "                                      current[seq+ \"_embedding_\" + embedding]))\n",
    "            sim_sent.append(sim_bin)\n",
    "        sim[sent] = sim_sent\n",
    "    return sim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use either gpt_image or gpt_sentence or gpt_meaning. method: sum of sequence image scores # winner:sbert + gpt_image\n",
    "modelmethod =  \"sbert\" #   \"meanLast4\" # \"meanLast\" # \"lastCLS\" # \"tfidf\" #  \n",
    "\n",
    "for sent in sent_types:\n",
    "    dataB[\"pred_binary_from_gpt_\" + sent] = dataB.apply(\n",
    "        lambda x:  np.argmax([sum(i) for i in compare_sequence_to_gpt(x,modelmethod)[sent]]), axis=1)\n",
    "    dataB[\"pred_binary_from_gpt_\" + sent] = dataB[\"pred_binary_from_gpt_\" + sent].replace([0,1],[ \"literal\",  \"idiomatic\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score (predict idiomaticity, compare sequence to gpt_meaning, embedding: sbert, method: max sum) 0.7\n",
      "accuracy score (predict idiomaticity, compare sequence to gpt_sentence, embedding: sbert, method: max sum) 0.6\n",
      "accuracy score (predict idiomaticity, compare sequence to gpt_image, embedding: sbert, method: max sum) 1.0\n"
     ]
    }
   ],
   "source": [
    "dataB_train = only_train(dataB)\n",
    "\n",
    "for sent in sent_types:\n",
    "    print(\"accuracy score (predict idiomaticity, compare sequence to gpt_\" + sent + \", embedding: \" + modelmethod + \", method: max sum)\", \n",
    "          str(accuracy_score(dataB_train[\"pred_binary_from_gpt_\" + sent], dataB_train[\"sentence_type\"])\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "literal      35\n",
       "idiomatic    25\n",
       "Name: pred_binary_from_gpt_image, dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataB[\"pred_binary_from_gpt_image\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### combine gpt_sentence/meaning/image for binary prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max of all sums # winner: sbert (0.95)\n",
    "modelmethod =  \"sbert\" #   \"meanLast4\" # \"lastCLS\" # \"meanLast\" # \"tfidf\" #\n",
    "\n",
    "\n",
    "for sent in sent_types:\n",
    "    dataB[\"pred_binary_from_gpt_all\"] = dataB.apply(\n",
    "        lambda x:  np.argmax([sum([sum(i[0]) for i in compare_sequence_to_gpt(x,modelmethod).values()]), \n",
    "                              sum([sum(i[1]) for i in compare_sequence_to_gpt(x,modelmethod).values()])]), axis=1)\n",
    "    dataB[\"pred_binary_from_gpt_all\"] = dataB[\"pred_binary_from_gpt_all\"].replace([0,1],[ \"literal\",  \"idiomatic\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score (predict idiomaticity, compare sequence to all gpt data, embedding: sbert, method: max sum) 0.95\n",
      "[including extended data] 0.8727272727272727\n"
     ]
    }
   ],
   "source": [
    "dataB_train = only_train(dataB)\n",
    "dataB_filtered = dataB[dataB['sentence_type'].notnull()]\n",
    "\n",
    "print(\"accuracy score (predict idiomaticity, compare sequence to all gpt data, embedding: \" + modelmethod + \", method: max sum)\", \n",
    "      str(accuracy_score(dataB_train[\"pred_binary_from_gpt_all\"], dataB_train[\"sentence_type\"])))\n",
    "print(\"[including extended data]\", \n",
    "      str(accuracy_score(dataB_filtered[\"pred_binary_from_gpt_all\"], dataB_filtered[\"sentence_type\"])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelmethod =  \"sbert\" #   \"meanLast4\" # \"lastCLS\" # \"meanLast\" # \"tfidf\" #  \n",
    "\n",
    "# max of max # winner sbert (1,0)\n",
    "for sent in sent_types:\n",
    "    dataB[\"pred_binary_from_gpt_all\"] = dataB.apply(\n",
    "        lambda x:  np.argmax([max([sum(i[0]) for i in compare_sequence_to_gpt(x,modelmethod).values()]), \n",
    "                              max([sum(i[1]) for i in compare_sequence_to_gpt(x,modelmethod).values()])]), axis=1)\n",
    "    dataB[\"pred_binary_from_gpt_all\"] = dataB[\"pred_binary_from_gpt_all\"].replace([0,1],[ \"literal\",  \"idiomatic\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score (predict idiomaticity, compare sequence to all gpt data, embedding: sbert, method: max max) 1.0\n",
      "[including extended data] 0.8909090909090909\n"
     ]
    }
   ],
   "source": [
    "dataB_train = only_train(dataB)\n",
    "dataB_filtered = dataB[dataB['sentence_type'].notnull()]\n",
    "\n",
    "print(\"accuracy score (predict idiomaticity, compare sequence to all gpt data, embedding: \" + modelmethod + \", method: max max)\", \n",
    "      str(accuracy_score(dataB_train[\"pred_binary_from_gpt_all\"], dataB_train[\"sentence_type\"])))\n",
    "print(\"[including extended data]\", \n",
    "      str(accuracy_score(dataB_filtered[\"pred_binary_from_gpt_all\"], dataB_filtered[\"sentence_type\"])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   idiomatic       1.00      1.00      1.00        13\n",
      "     literal       1.00      1.00      1.00         7\n",
      "\n",
      "    accuracy                           1.00        20\n",
      "   macro avg       1.00      1.00      1.00        20\n",
      "weighted avg       1.00      1.00      1.00        20\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   idiomatic       0.81      0.96      0.88        23\n",
      "     literal       0.96      0.84      0.90        32\n",
      "\n",
      "    accuracy                           0.89        55\n",
      "   macro avg       0.89      0.90      0.89        55\n",
      "weighted avg       0.90      0.89      0.89        55\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(dataB_train[\"pred_binary_from_gpt_all\"], dataB_train[\"sentence_type\"]))\n",
    "print(classification_report(dataB_filtered[\"pred_binary_from_gpt_all\"], dataB_filtered[\"sentence_type\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File zipped and saved as submission_EN.zip\n",
      "File zipped and saved as submission_xe.zip\n"
     ]
    }
   ],
   "source": [
    "make_submission(dataB, \"pred_binary_from_gpt_all\", \"pred_image_sbert\", \"Test\")\n",
    "make_submission(dataB, \"pred_binary_from_gpt_all\", \"pred_image_sbert\", \"Extended Evaluation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.06819207221269608, -0.08586937189102173, 0.16526855528354645]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with difference (compute differences between literal and idiomatic similarity values and add the differences. If sum>0 => literal)\n",
    "\n",
    "modelmethod =   \"sbert\" #   \"meanLast4\" # \"lastCLS\" # \"meanLast\" #    \"tfidf\" #\n",
    "\n",
    "dataB[\"pred_difference_from_gpt_all\"] = dataB.apply(\n",
    "        lambda x:  [sum(i[0]) - sum(i[1]) for i in compare_sequence_to_gpt(x,modelmethod).values()], axis=1)\n",
    "dataB[\"pred_difference_from_gpt_all\"][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dataB)):\n",
    "    old = dataB[\"pred_difference_from_gpt_all\"][i]\n",
    "    new = sum(old)\n",
    "    if new > 0:\n",
    "        new = \"literal\"\n",
    "    else:\n",
    "        new = \"idiomatic\"\n",
    "    dataB[\"pred_difference_from_gpt_all\"][i] = new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "literal      37\n",
       "idiomatic    23\n",
       "Name: pred_difference_from_gpt_all, dtype: int64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataB[\"pred_difference_from_gpt_all\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sbert\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   idiomatic       0.92      1.00      0.96        12\n",
      "     literal       1.00      0.88      0.93         8\n",
      "\n",
      "    accuracy                           0.95        20\n",
      "   macro avg       0.96      0.94      0.95        20\n",
      "weighted avg       0.95      0.95      0.95        20\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   idiomatic       0.78      0.95      0.86        22\n",
      "     literal       0.96      0.82      0.89        33\n",
      "\n",
      "    accuracy                           0.87        55\n",
      "   macro avg       0.87      0.89      0.87        55\n",
      "weighted avg       0.89      0.87      0.87        55\n",
      "\n",
      "accuracy score (predict idiomaticity, compare sequence to all gpt data, embedding: sbert, method: sum of differences) 0.95\n",
      "on extended data 0.8727272727272727\n"
     ]
    }
   ],
   "source": [
    "print(modelmethod)\n",
    "dataB_train = only_train(dataB)\n",
    "print(classification_report(dataB_train[\"pred_difference_from_gpt_all\"], dataB_train[\"sentence_type\"]))\n",
    "dataB_filtered = dataB[dataB['sentence_type'].notnull()]\n",
    "print(classification_report(dataB_filtered[\"pred_difference_from_gpt_all\"], dataB_filtered[\"sentence_type\"]))\n",
    "print(\"accuracy score (predict idiomaticity, compare sequence to all gpt data, embedding: sbert, method: sum of differences)\", accuracy_score(dataB_train[\"pred_difference_from_gpt_all\"], dataB_train[\"sentence_type\"]))\n",
    "print(\"on extended data\", accuracy_score(dataB_filtered[\"pred_difference_from_gpt_all\"], dataB_filtered[\"sentence_type\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File zipped and saved as submission_EN.zip\n",
      "File zipped and saved as submission_xe.zip\n"
     ]
    }
   ],
   "source": [
    "#test = make_submission(dataB, \"pred_binary_from_gpt_all\",\"pred_image_sbert\", \"Dev\")\n",
    "test = make_submission(dataB, \"pred_difference_from_gpt_all\",\"pred_image_sbert\", \"Test\")\n",
    "test = make_submission(dataB, \"pred_difference_from_gpt_all\",\"pred_image_sbert\", \"Extended Evaluation\")\n",
    "\n",
    "#test = make_submission(dataB, \"pred_binary_from_gpt_image\",\"pred_image_sbert\", \"Test\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
